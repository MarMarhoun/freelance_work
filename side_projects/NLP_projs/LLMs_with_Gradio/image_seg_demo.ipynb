{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFHGy9tKDFXbeqbpoblDPN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/LLMs_with_Gradio/image_seg_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical Image Segmentation: for X-ray and MRI Analysis\n",
        "\n",
        "To create an advanced image segmentation application using Gradio and Large Language Models (LLMs) from Hugging Face, we can follow these steps. The application will take an input image (such as an X-ray or MRI scan) from a URL, use a model to generate a segmentation mask, and then display both the mask and the segmented image.\n",
        "\n",
        "* Section 1: Medical Image Segmentation with LLMs\n",
        "* Section 2: Dynamic Medical Image Segmentation with LLMs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1777-sjxKual"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n",
        "!pip install gradio torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY-8oi-kK-hN",
        "outputId": "36109bbb-a84e-43db-b457-ca09cde7c42f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Medical Image Segmentation with LLMs"
      ],
      "metadata": {
        "id": "O8CPmIMWgwxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load the BLIP model and processor from Hugging Face for image captioning\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Define a fixed color palette for five regions\n",
        "color_palette = [\n",
        "    (0, 0, 0),    # Red\n",
        "    (0, 255, 0),    # Green\n",
        "    (0, 0, 255),    # Blue\n",
        "    (255, 255, 0),  # Yellow\n",
        "    (255, 165, 0)   # Orange\n",
        "]\n",
        "\n",
        "def segment_image(image):\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = image.convert(\"L\")\n",
        "    gray_array = np.array(gray_image)\n",
        "\n",
        "    # Create an empty array for the colored segmented image\n",
        "    colored_image = np.zeros((*gray_array.shape, 3), dtype=np.uint8)\n",
        "\n",
        "    # Define the thresholds for the five regions\n",
        "    thresholds = np.linspace(0, 255, num=6).astype(int)  # 5 regions\n",
        "\n",
        "    # Assign colors based on grayscale values\n",
        "    for i in range(5):\n",
        "        mask = (gray_array >= thresholds[i]) & (gray_array < thresholds[i + 1])\n",
        "        colored_image[mask] = color_palette[i]\n",
        "\n",
        "    # Convert the colored image back to PIL format\n",
        "    colored_image_pil = Image.fromarray(colored_image)\n",
        "\n",
        "    return image, gray_image, colored_image_pil  # Return original, grayscale, and colored images\n",
        "\n",
        "def generate_caption(image):\n",
        "    # Preprocess the image for the BLIP model\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate a caption\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs)\n",
        "\n",
        "    # Decode the generated caption\n",
        "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def update_image(image_url):\n",
        "    # This function fetches the image using request and returns it\n",
        "    response = requests.get(image_url)\n",
        "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\"<h1 style='text-align: center;'>Medical Image Segmentation with LLMs</h1>\")\n",
        "    gr.Markdown(\"Enter the URL of an X-ray or MRI image to generate a segmentation mask and the segmented image. \\n\\n\"\n",
        "                \"Example URLs:\\n\"\n",
        "                \"- X-ray: https://as2.ftcdn.net/v2/jpg/02/63/69/43/1000_F_263694305_nxVMuaTWhqa7uPAT0iUd5BaTzA6UU467.jpg \\n\"\n",
        "                \"- X-ray: https://as2.ftcdn.net/v2/jpg/01/97/03/19/1000_F_197031952_MGjgPGxw58GG6vQvA8xZh2f015WISGPe.jpg \\n\"\n",
        "                \"- X-ray: https://as1.ftcdn.net/v2/jpg/00/35/87/38/1000_F_35873899_xDcthI9k5yHGoL6B4jAyN5s1l2Ra3Weq.jpg \\n\"\n",
        "                \"- X-ray: https://as1.ftcdn.net/v2/jpg/02/66/34/72/1000_F_266347240_LmEx8algXY1sRE9stHPPtqJbGKKDtvh4.jpg \\n\"\n",
        "                \"- X-ray: https://as1.ftcdn.net/v2/jpg/04/19/34/16/1000_F_419341614_sFC7I0D9I5JSTeKwUcwAfNGzgEb3gdHO.jpg \\n\"\n",
        "                \"- MRI: https://www.kenhub.com/thumbor/zoz_XVCq44UFroH2ds6eoOUvdtA=/fit-in/800x1600/filters:watermark(/images/logo_url.png,-10,-10,0):background_color(FFFFFF):format(jpeg)/images/library/13517/ff.jpg \\n\"\n",
        "                \"- MRI: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/MRI_of_Human_Brain.jpg/330px-MRI_of_Human_Brain.jpg \\n\"\n",
        "                \"- MRI: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Normal_axial_T2-weighted_MR_image_of_the_brain.jpg/250px-Normal_axial_T2-weighted_MR_image_of_the_brain.jpg \\n\"\n",
        "                \"- MRI: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Atelectasia1.jpg/220px-Atelectasia1.jpg\"\n",
        "\n",
        "                )\n",
        "\n",
        "    # Input for image URL\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Input for image URL\n",
        "            image_url = gr.Textbox(label=\"Image URL\", placeholder=\"Enter the URL of the image\")\n",
        "            # Button for uploading the image\n",
        "            upload_button = gr.Button(\"Upload Image\")\n",
        "            # Call update_image function, which fetches and returns the image\n",
        "            upload_button.click(fn=update_image, inputs=image_url, outputs=uploaded_image)\n",
        "        with gr.Column():\n",
        "            # Output for the fetched image\n",
        "            uploaded_image = gr.Image(label=\"Uploaded Image\", type=\"pil\")\n",
        "            upload_button.click(fn=update_image, inputs=image_url, outputs=uploaded_image)\n",
        "\n",
        "\n",
        "    # Row for original, grayscale, and colored segmented images\n",
        "    with gr.Row():\n",
        "        original_image_output = gr.Image(label=\"Original Image\")\n",
        "        gray_image_output = gr.Image(label=\"Grayscale Image\")\n",
        "        colored_image_output = gr.Image(label=\"Colored Segmented Image\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "      run_button = gr.Button(\"Run Segmentation Test\")\n",
        "    # Define the action on button click for the run button\n",
        "      run_button.click(segment_image, inputs=uploaded_image, outputs=[original_image_output, gray_image_output, colored_image_output])\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "2u-42xEzQffm",
        "outputId": "815e4319-03d7-4331-da03-68cfcdec050f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e754ef7c7dd6fbc9f0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e754ef7c7dd6fbc9f0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Medical Image Segmentation with LLMs"
      ],
      "metadata": {
        "id": "bDcWOnc3ju4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load the BLIP model and processor from Hugging Face for image captioning\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Define a fixed color palette for regions\n",
        "color_palette = [\n",
        "    (0, 0, 0),    # Red\n",
        "    (0, 255, 0),    # Green\n",
        "    (0, 0, 255),    # Blue\n",
        "    (255, 255, 0),  # Yellow\n",
        "    (255, 165, 0)   # Orange\n",
        "]\n",
        "\n",
        "def segment_image(image, num_regions):\n",
        "    # Convert the image to grayscale\n",
        "    gray_image = image.convert(\"L\")\n",
        "    gray_array = np.array(gray_image)\n",
        "\n",
        "    # Create an empty array for the colored segmented image\n",
        "    colored_image = np.zeros((*gray_array.shape, 3), dtype=np.uint8)\n",
        "\n",
        "    # Define the thresholds for the specified number of regions\n",
        "    thresholds = np.linspace(0, 255, num=num_regions + 1).astype(int)  # num_regions + 1 for boundaries\n",
        "\n",
        "    # Assign colors based on grayscale values\n",
        "    for i in range(num_regions):\n",
        "        mask = (gray_array >= thresholds[i]) & (gray_array < thresholds[i + 1])\n",
        "        colored_image[mask] = color_palette[i % len(color_palette)]  # Cycle through colors if more regions than colors\n",
        "\n",
        "    # Convert the colored image back to PIL format\n",
        "    colored_image_pil = Image.fromarray(colored_image)\n",
        "\n",
        "    return image, gray_image, colored_image_pil  # Return original, grayscale, and colored images\n",
        "\n",
        "def generate_caption(image):\n",
        "    # Preprocess the image for the BLIP model\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate a caption\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs)\n",
        "\n",
        "    # Decode the generated caption\n",
        "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def update_image(image_url):\n",
        "    # This function fetches the image using request and returns it\n",
        "    response = requests.get(image_url)\n",
        "    image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\"<h1 style='text-align: center;'>Dynamic Medical Image Segmentation with LLMs</h1>\")\n",
        "    gr.Markdown(\"Enter the URL of an X-ray or MRI image to generate a segmentation mask and the segmented image. \\n\\n\"\n",
        "                \"Example URLs:\\n\"\n",
        "                \"- X-ray: https://as2.ftcdn.net/v2/jpg/02/63/69/43/1000_F_263694305_nxVMuaTWhqa7uPAT0iUd5BaTzA6UU467.jpg \\n\"\n",
        "                \"- X-ray: https://as2.ftcdn.net/v2/jpg/01/97/03/19/1000_F_197031952_MGjgPGxw58GG6vQvA8xZh2f015WISGPe.jpg \\n\"\n",
        "                \"- X-ray: https://as1.ftcdn.net/v2/jpg/00/35/87/38/1000_F_35873899_xDcthI9k5yHGoL6B4jAyN5s1l2Ra3Weq.jpg \\n\"\n",
        "                \"- X-ray: https://as1.ftcdn.net/v2/jpg/02/66/34/72/1000_F_266347240_LmEx8algXY1sRE9stHPPtqJbGKKDtvh4.jpg \\n\"\n",
        "                \"- X-ray: https://as1.ftcdn.net/v2/jpg/04/19/34/16/1000_F_419341614_sFC7I0D9I5JSTeKwUcwAfNGzgEb3gdHO.jpg \\n\"\n",
        "                \"- MRI: https://www.kenhub.com/thumbor/zoz_XVCq44UFroH2ds6eoOUvdtA=/fit-in/800x1600/filters:watermark(/images/logo_url.png,-10,-10,0):background_color(FFFFFF):format(jpeg)/images/library/13517/ff.jpg \\n\"\n",
        "                \"- MRI: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/MRI_of_Human_Brain.jpg/330px-MRI_of_Human_Brain.jpg \\n\"\n",
        "                \"- MRI: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Normal_axial_T2-weighted_MR_image_of_the_brain.jpg/250px-Normal_axial_T2-weighted_MR_image_of_the_brain.jpg \\n\"\n",
        "                \"- MRI: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Atelectasia1.jpg/220px-Atelectasia1.jpg\"\n",
        "\n",
        "                )\n",
        "    # Input for image URL\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Input for image URL\n",
        "            image_url = gr.Textbox(label=\"Image URL\", placeholder=\"Enter the URL of the image\")\n",
        "            # Button for uploading the image\n",
        "            upload_button = gr.Button(\"Upload Image\")\n",
        "            # Call update_image function, which fetches and returns the image\n",
        "            upload_button.click(fn=update_image, inputs=image_url, outputs=uploaded_image)\n",
        "            # Slider for number of regions\n",
        "            num_regions_slider = gr.Slider(minimum=2, maximum=20, step=1, label=\"Number of Regions\", value=5)\n",
        "\n",
        "        with gr.Column():\n",
        "            # Output for the fetched image\n",
        "            uploaded_image = gr.Image(label=\"Uploaded Image\", type=\"pil\")\n",
        "            upload_button.click(fn=update_image, inputs=image_url, outputs=uploaded_image)\n",
        "\n",
        "\n",
        "\n",
        "    # Row for original, grayscale, and colored segmented images\n",
        "    with gr.Row():\n",
        "        original_image_output = gr.Image(label=\"Original Image\")\n",
        "        gray_image_output = gr.Image(label=\"Grayscale Image\")\n",
        "        colored_image_output = gr.Image(label=\"Colored Segmented Image\")\n",
        "\n",
        "\n",
        "    with gr.Row():\n",
        "        run_button = gr.Button(\"Run Segmentation Test\")\n",
        "        # Define the action on button click for the run button\n",
        "        run_button.click(segment_image, inputs=[uploaded_image, num_regions_slider], outputs=[original_image_output, gray_image_output, colored_image_output])\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "8TX0zGFlWim0",
        "outputId": "b4bbd3a7-ff4e-4d78-9af6-bd20ddae6630"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f86dffa5429aee4756.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f86dffa5429aee4756.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1e6nQPjdffi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}