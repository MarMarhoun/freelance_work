{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2uWxkFaXoxu13LsAtGY5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/depression_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image depression prediction using deep learning, online dataset, XAI and deploy it into streamlit and tensorflow"
      ],
      "metadata": {
        "id": "H_dgGlksFKxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code for image depression prediction using deep learning, online dataset, XAI (Explainable Artificial Intelligence), and deploy it into Streamlit and TensorFlow, you can follow these steps:\n",
        "\n",
        "Data Collection: Gather a large, diverse, and labeled dataset of facial images representing different levels of depression. Online datasets like the 'AffectNet' or 'FAD' (Facial Affective Dataset) can be used.\n",
        "\n",
        "Data Preprocessing: Preprocess the images by resizing, normalization, and data augmentation to increase the model's performance and prevent overfitting.\n",
        "\n",
        "Model Architecture: Use a deep learning model based on Convolutional Neural Networks (CNNs) or Transfer Learning (e.g., VGG16, ResNet, or Inception) for facial expression recognition.\n",
        "\n",
        "Training: Train the model using the preprocessed dataset and optimize the model's hyperparameters using techniques like Grid Search or Random Search.\n",
        "\n",
        "Explainability: Implement XAI techniques like GradCAM, SHAP, or LIME to provide insights into the model's decision-making process and improve its interpretability.\n",
        "\n",
        "Streamlit Integration: Create a user-friendly Streamlit app that allows users to upload facial images, preprocess them, and predict the level of depression using the trained model. Display the XAI visualizations to explain the model's predictions.\n",
        "\n",
        "TensorFlow Deployment: Deploy the model and the Streamlit app on TensorFlow using TensorFlow Serving or TensorFlow Hub for easy integration with other applications.\n",
        "\n",
        "Here's a sample Streamlit app code for image depression prediction:"
      ],
      "metadata": {
        "id": "BzDIoLGPFPne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8c5-XwQEdaL"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = tf.keras.models.load_model('depression_model.h5')\n",
        "\n",
        "def predict_depression(image):\n",
        "    # Preprocess the image\n",
        "    image = image.resize((224, 224))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "\n",
        "    # Make the prediction\n",
        "    prediction = model.predict(image)\n",
        "    return prediction\n",
        "\n",
        "st.title(\"Image-based Depression Prediction\")\n",
        "\n",
        "# Upload an image\n",
        "uploaded_image = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"png\"])\n",
        "\n",
        "if uploaded_image is not None:\n",
        "    image = Image.open(uploaded_image)\n",
        "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "    # Predict depression\n",
        "    prediction = predict_depression(image)\n",
        "    st.write(\"Prediction:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to replace 'depression_model.h5' with the actual filename of your trained model. This is just a basic example; you can further enhance the app by adding XAI visualizations, user interface improvements, and more.\n",
        "\n",
        "Is this conversation helpful so far?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VHNrN-x0FflL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To enhance and add more advanced features and models to the existing code for image depression prediction using deep learning, XAI, and TensorFlow, and deploy it into Streamlit, consider the following steps:\n",
        "\n",
        "Data Augmentation: Implement data augmentation techniques like random rotation, flipping, and zooming to increase the size and diversity of your dataset. This can help improve the model's performance and prevent overfitting.\n",
        "\n",
        "Transfer Learning: Utilize pre-trained models like VGG16, ResNet, or Inception for feature extraction and fine-tune them for your specific task. This can help improve the model's accuracy and reduce training time.\n",
        "\n",
        "Ensemble Methods: Combine multiple models using ensemble methods like bagging, boosting, or stacking to improve the overall performance and robustness of your model.\n",
        "\n",
        "Hyperparameter Tuning: Optimize the model's hyperparameters using techniques like Grid Search, Random Search, or Bayesian Optimization to find the best combination of hyperparameters for your model.\n",
        "\n",
        "Explainability: Implement XAI techniques like GradCAM, SHAP, or LIME to provide insights into the model's decision-making process and improve its interpretability. This can help build trust in the model and its predictions.\n",
        "\n",
        "User Interface: Improve the Streamlit app's user interface by adding features like progress bars, alerts, and custom themes. This can help make the app more user-friendly and engaging.\n",
        "\n",
        "Model Monitoring and Maintenance: Implement model monitoring and maintenance strategies to ensure the model's performance remains consistent over time. This can include techniques like model versioning, A/B testing, and model retraining.\n",
        "\n",
        "Deployment: Deploy the Streamlit app and the model using cloud services like Heroku, AWS, or Google Cloud Platform. This can help make the app and the model accessible to a wider audience and improve their usability.\n",
        "\n",
        "Here's an example of how you can implement data augmentation and transfer learning in your code:"
      ],
      "metadata": {
        "id": "RiieLrgHF2rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the pre-trained model\n",
        "base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add a new top layer\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the dataset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'train_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    'validation_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "muShBBphFMJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to replace 'train_data' and 'validation_data' with the actual directories containing your training and validation datasets. This is just a basic example; you can further enhance the code by adding more advanced features and models as needed."
      ],
      "metadata": {
        "id": "okBCyUR4F8iL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhancement"
      ],
      "metadata": {
        "id": "DJabBXhdjnwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a code outline for image-based depression prediction using deep learning TensorFlow and Streamlit, along with XAI (eXplainable Artificial Intelligence) and an online dataset. I will then discuss how to add advanced features to this code.\n",
        "\n",
        "First, install the necessary libraries:\n",
        "\n"
      ],
      "metadata": {
        "id": "i9sgCBuClFiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow streamlit xgboost albumentations Pillow matplotlib sklearn pandas"
      ],
      "metadata": {
        "id": "pLRaqrkkF9u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, create a file named app.py and add the following code:\n",
        "\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from albumentations import *\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_dataset(data_url):\n",
        "    data = pd.read_csv(data_url)\n",
        "    images, labels = data.drop(columns=['id', 'diagnosis']).values, data['diagnosis'].values\n",
        "    images = images / 255.0\n",
        "    return images, labels\n",
        "\n",
        "# Model architecture\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Training function\n",
        "def train(model, images, labels, batch_size, epochs):\n",
        "    train_ds, val_ds = train_test_split(list(zip(images, labels)), test_size=0.2, random_state=42)\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_ds)).shuffle(len(train_ds)).batch(batch_size)\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((val_ds)).batch(batch_size)\n",
        "\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, image):\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    image = tf.image.resize(image, (48, 48))\n",
        "    return model.predict(image)\n",
        "\n",
        "# XAI function\n",
        "def xai(image, model):\n",
        "    importances = model.layers[-1].kernel[0]\n",
        "    importances = importances.reshape(48, 48)\n",
        "    plt.imshow(importances, cmap='viridis')\n",
        "    plt.title(\"Model Importance\")\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Image-based Depression Prediction\")\n",
        "    st.write(\"Upload an image and see if the person is depressed.\")\n",
        "\n",
        "    file_upload = st.file_uploader(\"Choose an image\", type=\"jpg\")\n",
        "\n",
        "    if file_upload:\n",
        "        image = np.array(Image.open(file_upload))\n",
        "        image = image.reshape(1, 48, 48, 1)\n",
        "\n",
        "        data_url = 'https://raw.githubusercontent.com/garythung/Depression-Detection/master/train.csv'\n",
        "        images, labels = load_dataset(data_url)\n",
        "        model = create_model()\n",
        "        train(model, images, labels, batch_size=32, epochs=10)\n",
        "\n",
        "        st.write(\"Model trained. Predicting...\")\n",
        "        prediction = predict(model, image)\n",
        "        st.write(\"Prediction: \", np.round(prediction[0][0], 2))\n",
        "\n",
        "        if st.checkbox(\"Show XAI\"):\n",
        "            st.write(\"Model Importance:\")\n",
        "            xai(image, model)\n",
        "            st.pyplot()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "aeOIOyuwlWLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's discuss how to add advanced features to this code:\n",
        "\n",
        "Data augmentation: Add data augmentation during training to improve the model's performance and robustness.\n",
        "\n",
        "Replace the train function with the following:"
      ],
      "metadata": {
        "id": "6OIeOElules7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, images, labels, batch_size, epochs):\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((images, labels)).shuffle(len(images)).batch(batch_size)\n",
        "    augmentation = Compose([\n",
        "        RandomRotate90(),\n",
        "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
        "        HorizontalFlip(),\n",
        "        VerticalFlip()\n",
        "    ])\n",
        "    train_ds = train_ds.map(lambda x, y: (augmentation(image=x)['image'], y))\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((images, labels)).batch(batch_size)\n",
        "\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "metadata": {
        "id": "qBeCUOgxlguW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning: Fine-tune a pre-trained model like MobileNet or ResNet for better performance.\n",
        "\n",
        "Replace the create_model function with the following:"
      ],
      "metadata": {
        "id": "GPietE1DlmYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(48, 48, 1))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0RbV3_-3lnuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model interpretability: Implement XAI (eXplainable Artificial Intelligence) techniques like SHAP or LIME to better understand the model's predictions.\n",
        "\n",
        "Add the following function to compute SHAP values for a given image:"
      ],
      "metadata": {
        "id": "gMhIHiDwlxmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "def shap_values(model, image):\n",
        "    explainer = shap.DeepExplainer(model, [image])\n",
        "    shap_values = explainer.shap_values(image)\n",
        "    return shap_values"
      ],
      "metadata": {
        "id": "XDPj--Erlvhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploying the model: To deploy the model in a production environment, you can use TensorFlow Serving, Flask, or Streamlit. In this example, you can use Streamlit to create a simple web application.\n",
        "\n",
        "Create a file named app.py and add the following code:"
      ],
      "metadata": {
        "id": "Iv45n2pwl3x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from albumentations import *\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_dataset(data_url):\n",
        "    data = pd.read_csv(data_url)\n",
        "    images, labels = data.drop(columns=['id', 'diagnosis']).values, data['diagnosis'].values\n",
        "    images = images / 255.0\n",
        "    return images, labels\n",
        "\n",
        "# Model architecture\n",
        "def create_model():\n",
        "    base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(48, 48, 1))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        base_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Training function\n",
        "def train(model, images, labels, batch_size, epochs):\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((images, labels)).shuffle(len(images)).batch(batch_size)\n",
        "    augmentation = Compose([\n",
        "        RandomRotate90(),\n",
        "        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2),\n",
        "        HorizontalFlip(),\n",
        "        VerticalFlip()\n",
        "    ])\n",
        "    train_ds = train_ds.map(lambda x, y: (augmentation(image=x)['image'], y))\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((images, labels)).batch(batch_size)\n",
        "\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, image):\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    image = tf.image.resize(image, (48, 48))\n",
        "    return model.predict(image)\n",
        "\n",
        "# SHAP values\n",
        "def shap_values(model, image):\n",
        "    explainer = shap.DeepExplainer(model, [image])\n",
        "    shap_values = explainer.shap_values(image)\n",
        "    return shap_values\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    st.title(\"Image-based Depression Prediction\")\n",
        "    st.write(\"Upload an image and see if the person is depressed.\")\n",
        "\n",
        "    file_upload = st.file_uploader(\"Choose an image\", type=\"jpg\")\n",
        "\n",
        "    if file_upload:\n",
        "        image = np.array(Image.open(file_upload))\n",
        "        image = image.reshape(1, 48, 48, 1)\n",
        "\n",
        "        data_url = 'https://raw.githubusercontent.com/garythung/Depression-Detection/master/train.csv'\n",
        "        images,"
      ],
      "metadata": {
        "id": "yuBI5VSil48m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_streamlit_app():\n",
        "    st.set_page_config(page_title=\"Image-based Depression Prediction\", page_icon=\":guardsman:\", layout=\"wide\")\n",
        "\n",
        "    st.title(\"Image-based Depression Prediction\")\n",
        "    st.write(\"Upload an image and see if the person is depressed.\")\n",
        "\n",
        "    file_upload = st.file_uploader(\"Choose an image\", type=\"jpg\")\n",
        "\n",
        "    if file_upload:\n",
        "        image = np.array(Image.open(file_upload))\n",
        "        image = image.reshape(1, 48, 48, 1)\n",
        "\n",
        "        class_name, shap_values = predict_and_explain(model, image)\n",
        "\n",
        "        st.write(f\"The predicted class is: {class_name}\")\n",
        "\n",
        "        # Display SHAP values\n",
        "        shap.image_plot(shap_values[0], -image[0])\n",
        "        st.pyplot()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_streamlit_app()"
      ],
      "metadata": {
        "id": "T1OewBCJmDPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's save the trained model and create a script to serve the model using TensorFlow Serving:\n",
        "\n"
      ],
      "metadata": {
        "id": "Z9TP1GvKmGr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_name):\n",
        "    model.save(os.path.join(model_name, 'model'))\n",
        "\n",
        "# Save the trained model\n",
        "save_model(model, 'depression_model')"
      ],
      "metadata": {
        "id": "B_03uMBUmH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can serve the model using TensorFlow Serving by creating a TensorFlow Serving Docker container and using the saved depression_model directory.\n",
        "\n",
        "This code now includes advanced features such as data augmentation, fine-tuning a pre-trained model, model interpretability using SHAP, and deploying the model into Streamlit and TensorFlow Serving.\n",
        "\n"
      ],
      "metadata": {
        "id": "XDtSkmJ0mPho"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sDB91Q3smQSb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}