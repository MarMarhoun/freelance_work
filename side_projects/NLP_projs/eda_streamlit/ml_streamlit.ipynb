{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNpnnlKwGt3Y5BTe9AXnisv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/ml_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkLs1XNwsT1q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project description:\n",
        "\n",
        "Here is an example of how you can train a multiclassification task using PyTorch and then deploy the model using Gradio. First, let's start with training the model. We will use the FashionMNIST dataset, which is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. We will use a simple convolutional neural network (CNN) architecture for this task. Here's the code to train the model:"
      ],
      "metadata": {
        "id": "kKBpVRMTsfVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fisrt example"
      ],
      "metadata": {
        "id": "7xTNP7vCtXF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load the FashionMNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / (i + 1)}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')"
      ],
      "metadata": {
        "id": "47ITZwEmsjcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's deploy the model using Gradio. Here's the code to create a simple web app using Gradio:"
      ],
      "metadata": {
        "id": "oNKZe3VNsoaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define a function to make predictions with the model\n",
        "def predict(image):\n",
        "    # Preprocess the image\n",
        "    image = image.resize((28, 28))\n",
        "    image = image.convert('L')\n",
        "    image = np.array(image)\n",
        "    image = image.reshape(1, 1, 28, 28)\n",
        "    image = image / 255.0\n",
        "    # Make a prediction\n",
        "    output = net(torch.from_numpy(image))\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    # Return the predicted class\n",
        "    return predicted.item()\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(fn=predict,\n",
        "                     inputs=gr.inputs.Image(type='pil', label='Upload an image'),\n",
        "                     outputs='label')\n",
        "\n",
        "# Launch the web app\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "ySF0XDMEspQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we define a function predict that takes an image as input, preprocesses it, makes a prediction using the trained model, and returns the predicted class. We then create a Gradio interface using the gr.Interface class, passing in the predict function, an image input component, and a label output component. Finally, we launch the web app using the launch method. When you run this code, Gradio will start a local web server and open up a browser window with the web app. You can then upload an image and see the predicted class. Note that this is just a simple example to get you started. Gradio offers many more features and options to customize your web app, such as input and output types, labels, and styles, as well as options to share your app, collect data, and integrate with other tools and services.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-7AymGCssWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yrk-BIZ3stQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Example"
      ],
      "metadata": {
        "id": "cWn78C4DtbpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio tensorflow"
      ],
      "metadata": {
        "id": "khh5uGk8tfs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "# Download human-readable labels for ImageNet.\n",
        "response = requests.get(\"https://git.io/JJkYN\")\n",
        "labels = response.text.split(\"\\n\")"
      ],
      "metadata": {
        "id": "yhGpr40Ptifn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_image(image):\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image.numpy().reshape(1, 224, 224, 3))\n",
        "    predictions = tf.keras.applications.mobilenet_v2.predict(image)\n",
        "    top_k = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top_k=3)[0]\n",
        "    confidences = {labels[i]: float(predictions[0][i]) for i in range(1000)}\n",
        "    return confidences, top_k"
      ],
      "metadata": {
        "id": "TALaQc33tkZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(fn=classify_image,\n",
        "                     inputs=gr.Image(shape=(224, 224)),\n",
        "                     outputs=gr.Label(num_top_classes=3),\n",
        "                     examples=[\"banana.jpg\", \"car.jpg\"])\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "OBpKbkHctnGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download human-readable labels for ImageNet.\n",
        "response = requests.get(\"https://git.io/JJkYN\")\n",
        "labels = response.text.split(\"\\n\")\n",
        "\n",
        "def classify_image(image):\n",
        "    if image is None or image.shape[0] == 0 or image.shape[1] == 0:\n",
        "        return {\"error\": \"Please upload a valid image file.\"}, None\n",
        "\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image.numpy().reshape(1, 224, 224, 3))\n",
        "    predictions = tf.keras.applications.mobilenet_v2.predict(image)\n",
        "    top_k = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top_k=3)[0]\n",
        "    confidences = {labels[i]: float(predictions[0][i]) for i in range(1000)}\n",
        "    return confidences, top_k\n",
        "\n",
        "iface = gr.Interface(fn=classify_image,\n",
        "                     inputs=gr.Image(shape=(224, 224), label=\"Upload an image\"),\n",
        "                     outputs=gr.Label(num_top_classes=3),\n",
        "                     examples=[\"banana.jpg\", \"car.jpg\"])\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "gD66dV_Stp1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third example"
      ],
      "metadata": {
        "id": "Ammr6hH2vAMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow streamlit pandas matplotlib sklearn"
      ],
      "metadata": {
        "id": "gPnagPYDvKMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you can create a Python script (e.g. app.py) with the following code:\n",
        "\n"
      ],
      "metadata": {
        "id": "OSw4fCDIvPjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read\\_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "X = df.drop('survived', axis=1).values\n",
        "y = df['survived'].values\n",
        "X_train, X_test, y_train, y_test = train\\_test\\_split(X, y, test\\_size=0.2, random\\_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit\\_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input\\_shape=(X\\_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary\\_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X\\_train, y\\_train, epochs=10, batch\\_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X\\_test, y\\_test)\n",
        "st.write('Test accuracy:', accuracy)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X\\_test)\n",
        "predictions = np.round(predictions)\n",
        "\n",
        "# Display the confusion matrix\n",
        "st.write('Confusion Matrix:')\n",
        "st.write(pd.crosstab(pd.Series(y\\_test, name='Actual'), pd.Series(predictions, name='Predicted')))\n",
        "\n",
        "# Display the first 5 rows of the test data\n",
        "st.write('First 5 rows of the test data:')\n",
        "st.write(df.iloc[:5])\n",
        "\n",
        "# Display the model summary\n",
        "st.write('Model summary:')\n",
        "model.summary(print\\_fn=lambda x: st.write(x))"
      ],
      "metadata": {
        "id": "sclQawF1vQm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script will load the Titanic dataset from a public URL, preprocess the data, define and train a simple neural network model, and then display the test accuracy, confusion matrix, first 5 rows of the test data, and the model summary.\n",
        "\n",
        "To deploy the model using Streamlit, you can run the following command in your terminal:"
      ],
      "metadata": {
        "id": "zowDVawevYiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "j2kO1F4VvZOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will start a local Streamlit server, and you can view the web app by opening a web browser and navigating to http://localhost:8501.\n",
        "\n",
        "Note that this is just a simple example, and you can customize the script to load your own dataset, preprocess the data differently, define a more complex model, and display additional information in the web app."
      ],
      "metadata": {
        "id": "N8ymWZEgvc8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fourth example\n",
        "\n",
        "Trained multiclassification task model using TensorFlow and Streamlit for deployment, you can follow these steps:\n",
        "\n",
        "Load the pre-trained model and its weights.\n",
        "Preprocess the input data to match the input shape and normalization requirements of the model.\n",
        "Add a new classification head to the model and compile it with a suitable loss function, optimizer, and metrics.\n",
        "Fine-tune the model by unfreezing some of the top layers and training them along with the new classification head.\n",
        "Create a Streamlit app to deploy the model and allow users to interact with it.\n",
        "Here's a sample code to get you started:"
      ],
      "metadata": {
        "id": "N4W8q7xVxpjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pre-trained model and its weights\n",
        "model = tf.keras.applications.MobileNetV2(input_shape=(160, 160, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Preprocess the input data\n",
        "@st.cache\n",
        "def preprocess_image(image):\n",
        "    img = Image.open(image)\n",
        "    img = img.resize((160, 160))\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_batch = np.expand_dims(img_array, axis=0)\n",
        "    return img_batch\n",
        "\n",
        "# Add a new classification head to the model and compile it\n",
        "num_classes = 2  # Change this to the number of classes in your dataset\n",
        "image_input = tf.keras.Input(shape=(160, 160, 3))\n",
        "x = model(image_input)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "output = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "new_model = tf.keras.Model(image_input, output)\n",
        "new_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "new_model.trainable = True\n",
        "\n",
        "# Load the training and validation datasets\n",
        "train_ds = ...  # Load your training dataset here\n",
        "val_ds = ...  # Load your validation dataset here\n",
        "\n",
        "# Fine-tune the model\n",
        "new_model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "# Create a Streamlit app\n",
        "def main():\n",
        "    st.title(\"Image Classification Model\")\n",
        "    image_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "    if image_file is not None:\n",
        "        img_batch = preprocess_image(image_file)\n",
        "        prediction = new_model.predict(img_batch)\n",
        "        class_index = np.argmax(prediction)\n",
        "        class_name = ('cat', 'dog')[class_index]\n",
        "        st.write(f\"The image is likely a {class_name}.\")\n",
        "        st.image(image_file, caption=f\"Classified as a {class_name}\", use_column_width=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "HDMmODyPv-Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis task using deep learning and the xai for interpretation and deploy it using streamlit and tensorflow\n",
        "\n",
        "Here is an example of code to train a deep learning model for a sentiment analysis task using Keras and TensorFlow, and use SHAP for interpretation. The model is then deployed using Streamlit.\n",
        "\n",
        "First, let's start by importing the necessary libraries:"
      ],
      "metadata": {
        "id": "OqJqJzxjRzsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import shap"
      ],
      "metadata": {
        "id": "U1W04QfNSDqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's load and preprocess the data. For this example, I will use the IMDB movie reviews dataset which is included in Keras."
      ],
      "metadata": {
        "id": "geCZXLdASG9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=10000)\n",
        "\n",
        "# Preprocess the data\n",
        "max_review_length = 120\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
        "\n",
        "# Convert labels to binary\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "7pUtHWNOSHqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's define and train the model. I will use a simple LSTM model for this example.\n",
        "\n"
      ],
      "metadata": {
        "id": "dBdIRHjLSMO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(10000, 128, input_length=max_review_length),\n",
        "    keras.layers.LSTM(128),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3)"
      ],
      "metadata": {
        "id": "p-dHyXeWSMv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's use SHAP to explain the model's predictions. I will use the Keras explainer provided by SHAP."
      ],
      "metadata": {
        "id": "0-nntz99SQna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.DeepExplainer(model, x_train[:10])\n",
        "shap_values = explainer.shap_values(x_train[:10])"
      ],
      "metadata": {
        "id": "8GTXVn2vSROV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's deploy the model using Streamlit.\n",
        "\n"
      ],
      "metadata": {
        "id": "LlnEMB0ASUWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "def predict(model, text):\n",
        "    # Preprocess the text\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    word_indices = [keras.datasets.imdb.get_word_index()[word] for word in words if word in keras.datasets.imdb.get_word_index().keys()]\n",
        "    word_indices = [wi - 3 for wi in word_indices if wi > 2]\n",
        "    word_indices = pad_sequences([word_indices], maxlen=max_review_length)\n",
        "    # Make prediction\n",
        "    prediction = model.predict(word_indices)\n",
        "    return prediction\n",
        "\n",
        "st.title(\"Sentiment Analysis\")\n",
        "st.write(\"Enter a movie review to classify its sentiment\")\n",
        "\n",
        "text = st.text_input(\"Review\")\n",
        "if st.button(\"Predict\"):\n",
        "    with st.spinner(\"Predicting...\"):\n",
        "        prediction = predict(model, text)\n",
        "        if prediction > 0.5:\n",
        "            st.write(\"Positive review\")\n",
        "        else:\n",
        "            st.write(\"Negative review\")"
      ],
      "metadata": {
        "id": "Jtec0hnrSVCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a basic example, you can add more feature like visualization, explanation, and more.\n",
        "\n",
        "Please note that this is a simplified example and you may need to adjust the code to fit your specific use case. Also, you need to install the necessary libraries and dependencies before running the code"
      ],
      "metadata": {
        "id": "ohtzw9ZJSY3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enhancement\n",
        "To enhance and add more advanced features to a deep learning model for sentiment analysis, and deploy it using Streamlit, TensorFlow, and XAI for interpretation, you can follow the steps below:\n",
        "\n",
        "Install necessary libraries:"
      ],
      "metadata": {
        "id": "MCsbaygbSroR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries:\n",
        "\n",
        "!pip install streamlit tensorflow transformers xai-toolkit"
      ],
      "metadata": {
        "id": "5aJZz5iYSZig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new Python file (e.g., app.py) and import the necessary libraries:\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from xai_toolkit import Explanation\n",
        "\n",
        "# Load the pre-trained model and tokenizer:\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "NB-TtgIkS6yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess the input text and generate predictions:\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probabilities)\n",
        "    confidence = probabilities[0, predicted_class].item()\n",
        "    return predicted_class.item(), confidence\n",
        "\n",
        "# Define a function to generate explanations using XAI:\n",
        "\n",
        "def generate_explanation(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    explanation = Explanation(model, inputs)\n",
        "    return explanation.get_integrated_gradients()\n",
        "\n",
        "\n",
        "# Create the Streamlit app:\n",
        "st.title(\"Sentiment Analysis with XAI\")\n",
        "text = st.text_input(\"Enter your text:\")\n",
        "\n",
        "if text:\n",
        "    class_id, confidence = predict_sentiment(text)\n",
        "    sentiment = \"Positive\" if class_id == 0 else \"Negative\"\n",
        "    st.write(f\"Sentiment: {sentiment} (Confidence: {confidence:.2f})\")\n",
        "\n",
        "    explanation = generate_explanation(text)\n",
        "    st.write(\"Explanation:\")\n",
        "    st.image(explanation.get_heatmap(), caption=\"Heatmap\", use_column_width=True)"
      ],
      "metadata": {
        "id": "8N-Dl_-XTDQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy the app using Streamlit Sharing:\n",
        "\n",
        "streamlit run app.py --browser.server.headless"
      ],
      "metadata": {
        "id": "zC5_cfkaTR39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After deploying, you can share the link with the Product Manager or any other non-technical person. They can enter their text, and the model will classify it as 'POSITIVE' or 'NEGATIVE' sentiment, along with a confidence score. Additionally, the XAI-generated explanation will help them understand the model's decision"
      ],
      "metadata": {
        "id": "ZYq8XIs9TYBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhancement 2"
      ],
      "metadata": {
        "id": "DY7p8OSMV30D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as\n",
        "import pandas pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from pprint import pprint\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Load the dataset\n",
        "@st.cache\n",
        "def load_data():\n",
        "    url = \"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\"\n",
        "    data = pd.read_csv(url)\n",
        "    return data\n",
        "\n",
        "data = load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(data):\n",
        "    # Remove HTML tags\n",
        "    data.review = data.review.str.replace(r'<.*?>', '')\n",
        "\n",
        "    # Remove non-alphabetic characters\n",
        "    data.review = data.review.str.replace(r'[^a-zA-Z\\s]', '')\n",
        "\n",
        "    # Convert to lowercase\n",
        "    data.review = data.review.str.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    data.review = data.review.apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords))\n",
        "\n",
        "    # Tokenize the reviews\n",
        "    max_fatures = 2000\n",
        "    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "    tokenizer.fit_on_texts(data.review)\n",
        "    sequences = tokenizer.texts_to_sequences(data.review)\n",
        "    data.review = sequences\n",
        "\n",
        "    # Pad sequences\n",
        "    X = pad_sequences(data.review, maxlen=max_fatures)\n",
        "    return X, tokenizer\n",
        "\n",
        "X, tokenizer = preprocess_data(data)\n",
        "\n",
        "# Preprocess the labels\n",
        "le = LabelEncoder()\n",
        "data.sentiment = le.fit_transform(data.sentiment)\n",
        "y = to_categorical(data.sentiment)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "def define_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Embedding(max_fatures, 128, input_length=X.shape[1]))\n",
        "    model.add(SpatialDropout1D(0.4))\n",
        "    model.add(LSTM(192, dropout=0.4, recurrent_dropout=0.4))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# Define a function for predicting the sentiment of a review\n",
        "def predict_sentiment(review):\n",
        "    review = [tokenizer.word_index[word] for word in review.split() if word in tokenizer.word_index]\n",
        "    review = pad_sequences([review], maxlen=X.shape[1])\n",
        "    prediction = model.predict(review)\n",
        "    if prediction > 0.5:\n",
        "        return 'Positive'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "# Define a function for interpreting the model's predictions using SHAP\n",
        "def explain_prediction(review):\n",
        "    explainer = shap.DeepExplainer(model, X_train)\n",
        "    shap_values = explainer.shap_values(review)\n",
        "    return shap_values\n",
        "\n",
        "# Define a function for visualizing the word cloud of a review\n",
        "def visualize_word_cloud(review):\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    review = \" \".join(review for review in review.split() if review not in stopwords)\n",
        "    wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(review)\n",
        "    fig = go.Figure(data=go.Scatter(x=0, y=0, mode='lines', marker=dict(size=10, color='850000', opacity=0.6)))\n",
        "    fig.add_shape(type='circle', xref='x', yref='y', x0=0, x1=1, y0=0, y1=1, line=dict(color='850000'))\n",
        "    fig.add_trace(go.Scatter(x=wordcloud.words[:, 0], y=wordcloud.words[:, 1], mode='text', text=wordcloud.words[:, 0], textfont=dict(size=wordcloud.words[:, 2])))\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "# Define the Streamlit app\n",
        "def main():\n",
        "    st.title('Sentiment Analysis using Deep Learning and XAI')\n",
        "\n",
        "    # Text input for user review\n",
        "    user_review = st.text_area('Enter your review here:')\n",
        "\n",
        "    if st.button('Predict Sentiment'):\n",
        "        with st.spinner('Predicting sentiment...'):\n",
        "            sentiment = predict_sentiment(user_review)\n",
        "            st.success(f'The sentiment of the review is {sentiment}.')\n",
        "\n",
        "            # Interpret the model's predictions\n",
        "            if st.checkbox('Interpret Prediction'):\n",
        "                with st.spinner('Interpreting prediction...'):\n",
        "                    shap_values = explain_prediction(pad_sequences([user_review], maxlen=X.shape[1]))\n",
        "                    st.write('SHAP values:')\n",
        "                    st.write(shap_values)\n",
        "\n",
        "                    # Visualize the word cloud\n",
        "                    visualize_word_cloud(user_review)"
      ],
      "metadata": {
        "id": "S_vR5LCuTSVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "from wordcloud import WordCloud\n",
        "import plotly.graph_objects as go\n",
        "import streamlit as st\n",
        "\n",
        "# Define a function for loading the data\n",
        "def load_data(file):\n",
        "    df = pd.read_csv(file)\n",
        "    df['Text'] = df['Text'].apply(lambda x: x.lower())\n",
        "    df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in ('.', ',', '!', '?', ';', ':')])))\n",
        "    return df\n",
        "\n",
        "# Define a function for building the model\n",
        "def build_model(max_features, embedding_dims, maxlen):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(max_features, embedding_dims, input_length=maxlen),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define a function for predicting the sentiment\n",
        "def predict_sentiment(text):\n",
        "    predictions = model.predict(np.array([text]))\n",
        "    return 'Positive' if predictions[0][0] > 0.5 else 'Negative'\n",
        "\n",
        "# Define a function for interpreting the model's predictions\n",
        "def explain_prediction(text):\n",
        "    # Explain predictions using SHAP\n",
        "    explainer = shap.Explainer(model, X)\n",
        "    shap_values = explainer.shap_values(text)\n",
        "    return shap_values\n",
        "\n",
        "# Define a function for visualizing the word cloud of a review\n",
        "def visualize_word_cloud(review):\n",
        "    stopwords = set(stopwords.words('english'))\n",
        "    review = \" \".join(review for review in review.split() if review not in stopwords)\n",
        "    wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(review)\n",
        "    fig = go.Figure(data=go.Scatter(x=0, y=0, mode='lines', marker=dict(size=10, color='850000', opacity=0.6)))\n",
        "    fig.add_shape(type='circle', xref='x', yref='y', x0=0, x1=1, y0=0, y1=1, line=dict(color='850000'))\n",
        "    fig.add_trace(go.Scatter(x=wordcloud.words[:, 0], y=wordcloud.words[:, 1], mode='text', text=wordcloud.words[:, 0], textfont=dict(size=wordcloud.words[:, 2])))\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "# Define the Streamlit app\n",
        "def main():\n",
        "    st.title('Sentiment Analysis using Deep Learning and XAI')\n",
        "\n",
        "    # Load the data\n",
        "    data = load_data('data.csv')\n",
        "\n",
        "    # Create the training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Tokenize the data and pad the sequences\n",
        "    tokenizer = Tokenizer(num_words=5000, oov_token='OOV')\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train = tokenizer.texts_to_sequences(X_"
      ],
      "metadata": {
        "id": "N_jM_6GaWni1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis task using deep learning and the xai for interpretation and deploy it using Gradio and tensorflow\n",
        "\n",
        "Here is an example of how you can train a deep learning model for a sentiment analysis task using TensorFlow and the Keras API, interpret the model using XAI techniques, and deploy it using Gradio:\n",
        "\n",
        "First, let's start by importing the necessary libraries:"
      ],
      "metadata": {
        "id": "ks2uudBVoXhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import shap\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "T07qf13hocAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, let's load the dataset. For this example, I will use the IMDB movie reviews dataset, which is a binary sentiment analysis dataset.\n",
        "\n",
        "data = pd.read_csv('imdb_reviews.csv')\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = data[:20000]\n",
        "test_data = data[20000:]\n",
        "\n",
        "# Define the labels\n",
        "train_labels = train_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
        "test_labels = test_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
        "\n",
        "# Remove the sentiment column\n",
        "train_data = train_data.drop('sentiment', axis=1)\n",
        "test_data = test_data.drop('sentiment', axis=1)"
      ],
      "metadata": {
        "id": "bu_4AK8eok1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's preprocess the text data by tokenizing the words and converting them into sequences. We will also pad the sequences so that they all have the same length."
      ],
      "metadata": {
        "id": "f7uNnfBaopjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['review'])\n",
        "\n",
        "# Convert the words to sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data['review'])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['review'])\n",
        "\n",
        "# Pad the sequences\n",
        "max_sequence_length = 1000\n",
        "\n",
        "x_train = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "x_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\n"
      ],
      "metadata": {
        "id": "kdHRaxvJorxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, let's define the model architecture. For this example, I will use a simple LSTM model.\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=max_sequence_length),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, train_labels, validation_data=(x_test, test_labels), epochs=10)"
      ],
      "metadata": {
        "id": "8sQ2iqQMoxRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's interpret the model using SHAP (SHapley Additive exPlanations). SHAP is a method for interpreting the output of machine learning models by quantifying the importance of each feature in making a specific prediction."
      ],
      "metadata": {
        "id": "GMc6kTgMo1j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate SHAP values for the training set\n",
        "explainer = shap.DeepExplainer(model, x_train)\n",
        "shap_values = explainer.shap_values(x_train)\n",
        "\n",
        "# Visualize the SHAP values for the first 10 samples\n",
        "shap.force_plot(explainer.expected_value[0], shap_values[0][:10], train_data['review'][:10])"
      ],
      "metadata": {
        "id": "ChZN3Yo2o2Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's deploy the model using Gradio. Gradio is a Python library for creating machine learning demos that can be shared with others."
      ],
      "metadata": {
        "id": "RdTUT4_Ao5oV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function that takes in the input and returns the prediction\n",
        "def predict(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    return 'Positive' if prediction > 0.5 else 'Negative'\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(fn=predict, inputs='text', outputs='label')\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "ygLEbZTFo6TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create a web interface where users can input text and get a sentiment analysis prediction. The interface will look something like this:\n",
        "\n",
        "Gradio Interface\n",
        "\n",
        "And that's it! You have now trained a deep learning model for a sentiment analysis task, interpreted the model using XAI techniques, and deployed it using Gradio. Of course, this is just a simple example, and there are many ways to customize and improve the model and the interface. But hopefully, this gives you a good starting point for building your own sentiment analysis model using TensorFlow and Gradio"
      ],
      "metadata": {
        "id": "0gXlLs-co_oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional code\n",
        "import tensorflow as tf\n",
        "from tensorflow_serving.apis import predict_pb2\n",
        "from tensorflow_serving.client import predict_client\n",
        "\n",
        "# Create a predict client\n",
        "hostport = 'localhost:9000'\n",
        "predict_client = predict_client.PredictClient(hostport)\n",
        "\n",
        "# Define the model name and signature name\n",
        "model_name = 'my_sentiment_analysis_model'\n",
        "signature_name = 'serving_default'\n",
        "\n",
        "# Load the model into memory\n",
        "model = tf.keras.models.load_model('my_sentiment_analysis_model.h5')\n",
        "\n",
        "# Define the input data\n",
        "input_data = {'text': ['I love this product!']}\n",
        "\n",
        "# Convert the input data to a serialized TensorFlow Serving message\n",
        "input_data_serialized = tf.compat.as_bytes(tf.io.serialize_tensor(tf.convert_to_tensor(input_data)))\n",
        "\n",
        "# Define the request message\n",
        "request = predict_pb2.PredictRequest()\n",
        "request.model_spec.name = model_name\n",
        "request.model_spec.signature_name = signature_name\n",
        "request.inputs['input'].CopyFrom(tf.make_tensor_proto(input_data_serialized))\n",
        "\n",
        "# Send the request to the TensorFlow Serving server\n",
        "response = predict_client.Predict(request, timeout=10.0)\n",
        "\n",
        "# Extract the output data from the response message\n",
        "output_data = tf.make_ndarray(response.outputs['output'])\n",
        "\n",
        "# Print the output data\n",
        "print(output_data)"
      ],
      "metadata": {
        "id": "M3mglsKPpAoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhancement"
      ],
      "metadata": {
        "id": "l4jCJu-JroEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define functions for data preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    text = ' '.join(words)\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "\n",
        "# Preprocess the text data\n",
        "train_data['text'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['text'] = test_data['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "J7O0dsrbpE7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering:\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Define a TfidfVectorizer with custom parameters\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 3))\n",
        "\n",
        "# Generate features for the text data\n",
        "train_features = vectorizer.fit_transform(train_data['text'])\n",
        "test_features = vectorizer.transform(test_data['text'])"
      ],
      "metadata": {
        "id": "IZA-tkX6rtvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture:\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Load a pre-trained BERT model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_features) * epochs)\n",
        "\n",
        "# Define training function\n",
        "def train(model, optimizer, scheduler, train_features, train_labels, epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(train_features, labels=train_labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    return loss.item()\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(model, test_features, test_labels):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(test_features)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy_score(test_labels, preds)\n",
        "        f1 = f1_score(test_labels, preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, optimizer, scheduler, train_features, train_labels, epochs)\n",
        "    test_acc, test_f1 = evaluate(model, test_features, test_labels)\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.3f}, Test Acc: {test_acc:.3f}, Test F1: {test_f1:.3f}')"
      ],
      "metadata": {
        "id": "kOR_B_Eer2-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explainability (XAI):\n",
        "\n",
        "import shap\n",
        "\n",
        "# Generate SHAP values for the test data\n",
        "explainer = shap.DeepExplainer(model, test_features)\n",
        "shap_values = explainer.shap_values(test_features)\n",
        "\n",
        "# Visualize the SHAP values for a few instances\n",
        "for i in range(10):\n",
        "    shap.force_plot(explainer.expected_value[i], shap_values[i], test_features.numpy()[i])\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HED3aFH7r7T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deployment"
      ],
      "metadata": {
        "id": "QK5u2D2SscMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers torch scipy"
      ],
      "metadata": {
        "id": "CrTkbIissfKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "17LLyGt5shvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"KAITANY/finetuned-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "NCsgI0Tzsj9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Preprocess text (username and link placeholders)\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)"
      ],
      "metadata": {
        "id": "YyicIBUusl_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_analysis(text):\n",
        "    text = preprocess(text)\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    # Make a prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Get the predicted class probabilities\n",
        "    scores = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
        "    # Map the scores to labels\n",
        "    labels = ['Negative', 'Neutral', 'Positive']\n",
        "    scores_dict = {label: score for label, score in zip(labels, scores)}\n",
        "    return scores_dict"
      ],
      "metadata": {
        "id": "9NlfzNA3sn9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title = \"Sentiment Analysis Application\\n\\n\\nThis application assesses if a twitter post relating to vaccination is positive,neutral or negative\"\n",
        "demo = gr.Interface(fn=sentiment_analysis,\n",
        "                   inputs=gr.Textbox(placeholder=\"Write your tweet here...\"),\n",
        "                   outputs=gr.Label(num_top_classes=3),\n",
        "                   examples=[[\"The Vaccine is harmful!\"],\n",
        "                              [\"I cant believe people don't vaccinate their kids\"],\n",
        "                              [\"FDA think just not worth the AE unfortunately\"],\n",
        "                              [\"For a vaccine given to healthy\"]],\n",
        "                   title=title)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "ZSFX3wZZsqAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretability with XAI**\n",
        "\n",
        "To interpret the predictions made by the model, we can use explainable AI (XAI) techniques. One such technique is Local Interpretable Model-agnostic Explanations (LIME), which can help us understand how the model is making its predictions.\n",
        "\n",
        "Let's install the necessary library:"
      ],
      "metadata": {
        "id": "RIWpS0eSstlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alibi"
      ],
      "metadata": {
        "id": "NHQv3AoBswE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from alibi.explainers import LIME, TaskType"
      ],
      "metadata": {
        "id": "Elx4TGmys1Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define a function to explain the predictions:\n",
        "\n",
        "def explain_prediction(text):\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "    # Make a prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Get the predicted class probabilities\n",
        "    scores = torch.softmax(outputs.logits, dim=1).tolist()[0]\n",
        "    # Map the scores to labels\n",
        "    labels = ['Negative', 'Neutral', 'Positive']\n",
        "    # Explain the prediction using LIME\n",
        "    explainer = LIME(model, task=\"classification\", feature_type=\"text\", categorical_features=[])\n",
        "    explanation = explainer.explain(text, labels)\n",
        "    return explanation"
      ],
      "metadata": {
        "id": "Eq0ATmZCs8Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's modify the Gradio interface to include the explanation:\n",
        "\n",
        "title = \"Sentiment Analysis Application\\n\\n\\nThis application assesses if a twitter post relating to vaccination is positive,neutral or negative\"\n",
        "demo = gr.Interface(fn=sentiment_analysis,\n",
        "                   inputs=gr.Textbox(placeholder=\"Write your tweet here...\"),\n",
        "                   outputs=gr.Label(num_top_classes=3),\n",
        "                   examples=[[\"The Vaccine is harmful!\"],\n",
        "                              [\"I cant believe people don't vaccinate their kids\"],\n",
        "                              [\"FDA think just not worth the AE unfortunately\"],\n",
        "                              [\"For a vaccine given to healthy\"]],\n",
        "                   output_types=[gr.outputs.Label(num_top_classes=3)],\n",
        "                   output_styles=[{\"type\": \"prediction\", \"prediction_type\": \"probability\"}],\n",
        "                   description=\"This application assesses if a twitter post relating to vaccination is positive,neutral or negative. You can also see the explanation for the prediction.\",\n",
        "                   article=\"The explanation for the prediction is generated using Local Interpretable Model-agnostic Explanations (LIME). It shows the importance of each word in the input text for the predicted sentiment.\",\n",
        "                   sidebar=\"Explain Prediction\",\n",
        "                   sidebar_outputs=[gr.outputs.Textbox(label=\"Explanation\")],\n",
        "                   sidebar_function=explain_prediction,\n",
        "                   title=title)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "D8bUjgF5s8Xo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}