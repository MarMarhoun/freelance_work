{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaaCNyqtI6NbVu1mPsWW68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/outlier_detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier detection using streamlit and tensorflow\n",
        "\n",
        "the code for outlier detection using Streamlit and TensorFlow, you can create a user-friendly app that allows users to upload a dataset, preprocess the data, train an autoencoder for anomaly detection, and display the results. Here's an example of how you can create an app using the previously provided code as a starting point:\n",
        "\n",
        "First, install Streamlit if you haven't already: `!pip install streamlit`\n",
        "\n",
        "\n",
        "Next, create a new Python script and import the required libraries:"
      ],
      "metadata": {
        "id": "7xVpxjB7WLmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "import streamlit as st\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "RO0CB7TgWO84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define helper functions for loading data and creating the autoencoder:\n"
      ],
      "metadata": {
        "id": "7U4UYTtHWx49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    # Add your function to load and preprocess the data\n",
        "\n",
        "def create_autoencoder(input_shape, latent_dim):\n",
        "    # Add your function to create and compile the autoencoder"
      ],
      "metadata": {
        "id": "7XJBZl4YWuip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_app():\n",
        "    st.title(\"Outlier Detection using Autoencoder and TensorFlow\")\n",
        "\n",
        "    # File upload\n",
        "    uploaded_file = st.file_uploader(\"Upload dataset (csv or image folder)\", type=\"csv|zip\")\n",
        "    if uploaded_file is not None:\n",
        "        data_dir = \"./data\"\n",
        "        if not os.path.exists(data_dir):\n",
        "            os.makedirs(data_dir)\n",
        "        with open(os.path.join(data_dir, \"data.csv\"), \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "\n",
        "        # Load data\n",
        "        data = load_data(data_dir)\n",
        "\n",
        "        # Preprocess data\n",
        "        # Add your preprocessing steps here\n",
        "\n",
        "        # Train-test split\n",
        "        X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Train autoencoder\n",
        "        input_shape = X_train.shape[1:]\n",
        "        latent_dim = 16\n",
        "        autoencoder = create_autoencoder(input_shape, latent_dim)\n",
        "        autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, validation_data=(X_test, X_test))\n",
        "\n",
        "        # Evaluate the autoencoder\n",
        "        X_test_reconstructed = autoencoder.predict(X_test)\n",
        "        distances = pairwise_distances(X_test, X_test_reconstructed)\n",
        "\n",
        "        # Display results\n",
        "        st.header(\"Results\")\n",
        "        st.write(\"Mean reconstruction error:\", np.mean(distances))\n",
        "        st.write(\"Standard deviation of the reconstruction error:\", np.std(distances))\n",
        "\n",
        "        st.subheader(\"Top 10 images with the highest reconstruction errors:\")\n",
        "        for idx in distances.argsort()[::-1][:10]:\n",
        "            st.image(Image.open(os.path.join(data_dir, \"images\", f\"{idx}.png\")), caption=f\"Original image\")\n",
        "            st.image(Image.open(os.path.join(data_dir, \"images\", f\"{idx}_reconstructed.png\")), caption=f\"Reconstructed image\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_app()"
      ],
      "metadata": {
        "id": "3gW_E5zUW93Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the Streamlit app:\n"
      ],
      "metadata": {
        "id": "5W7vin1QXDxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "cAWAN5aOXlXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example provides a simple Streamlit app for outlier detection using an autoencoder. You can further customize the app by adding more features, such as different preprocessing options, model architectures, and visualizations.\n",
        "\n"
      ],
      "metadata": {
        "id": "KkgtndzkXofK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYxyCBmSXpXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the provided content, I can see that the initial code focused on creating a Streamlit app for a REST API for outlier detection. I'll build upon that code and add advanced features using TensorFlow for outlier detection.\n",
        "\n",
        "First, make sure you have TensorFlow installed: pip install tensorflow.\n",
        "\n",
        "Next, create a new directory tf_outlier_detection and inside, create a Python script named app.py. After that, create another directory named models.\n",
        "\n",
        "Here's the enhanced Streamlit app code:"
      ],
      "metadata": {
        "id": "KSs8HQoYY_Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Initialize model and load saved weights\n",
        "@st.experimental_memo\n",
        "def load_model(model_path):\n",
        "    model = Model(inputs=Input(shape=(784,)), outputs=Dense(32, activation='relu')(Dense(16, activation='relu')))\n",
        "    model.load_weights(model_path)\n",
        "    return model\n",
        "\n",
        "# Detect outliers using TensorFlow Autoencoder\n",
        "def detect_outliers_tf(model, data):\n",
        "    data = np.reshape(data, (1, 784))\n",
        "    output = model.predict(data)\n",
        "    reconstructed = np.reshape(output, (28, 28))\n",
        "    return np.mean(pairwise_distances(np.reshape(data, (784,)), reconstructed))\n",
        "\n",
        "# Define Streamlit app\n",
        "def main():\n",
        "    st.title(\"Outlier Detection using Autoencoder and TensorFlow\")\n",
        "\n",
        "    # Method selection\n",
        "    method = st.selectbox(label=\"Choose the method\", options=[\"TensorFlow Autoencoder\"])\n",
        "\n",
        "    if method == \"TensorFlow Autoencoder\":\n",
        "        st.write(\"### TensorFlow Autoencoder\")\n",
        "\n",
        "        # Load pre-trained model\n",
        "        model_path = \"./models/tf_autoencoder_model.h5\"\n",
        "        model = load_model(model_path)\n",
        "\n",
        "        # Image upload\n",
        "        uploaded_image = st.file_uploader(\"Upload an image\", type=\"jpg|jpeg|png\")\n",
        "        if uploaded_image is not None:\n",
        "            img = Image.open(uploaded_image)\n",
        "            img = img.resize((28, 28))\n",
        "            img_array = np.array(img)\n",
        "            img_array = img_array.reshape(-1, 784)\n",
        "\n",
        "            # Detect outliers and display result\n",
        "            distance = detect_outliers_tf(model, img_array)\n",
        "            st.subheader(\"Result\")\n",
        "            st.write(f\"Reconstruction error: {distance:.3f}\")\n",
        "\n",
        "            # Display image and reconstructed image\n",
        "            col1, col2 = st.beta_columns(2)\n",
        "            with col1:\n",
        "                st.write(\"Original image\")\n",
        "                st.image(img, caption=\"Uploaded Image\", use_column_width=True)\n",
        "            with col2:\n",
        "                st.write(\"Reconstructed image\")\n",
        "                st.image(img.resize((28, 28)), caption=\"Reconstructed Image\", use_column_width=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "NeUXBBkXZAIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create the TensorFlow autoencoder model and save the weights:\n",
        "\n"
      ],
      "metadata": {
        "id": "mPABXJqqZH06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import streamlit as st\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Download pre-trained model weights\n",
        "def download_weights(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        response = requests.get(url)\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "# Download dataset\n",
        "def download_dataset(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        response = requests.get(url)\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(filename):\n",
        "    data = np.load(filename)\n",
        "    return data['images'], data['labels']\n",
        "\n",
        "# Build the autoencoder model\n",
        "def build_autoencoder(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    encoded = Dense(16, activation='relu')(input_layer)\n",
        "    decoded = Dense(input_shape[0] * input_shape[1], activation='sigmoid')(encoded)\n",
        "    output_layer = tf.keras.layers.Reshape(input_shape)(decoded)\n",
        "    model = Model(input_layer, output_layer)\n",
        "    return model\n",
        "\n",
        "# Train the autoencoder model\n",
        "def train_autoencoder(model, dataset, batch_size):\n",
        "    x_train, y_train = dataset\n",
        "    x_train = x_train.reshape(-1, 784)\n",
        "    callback = ModelCheckpoint('autoencoder_weights.h5', save_best_only=True)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    model.fit(x_train, x_train, epochs=50, batch_size=batch_size, callbacks=[callback], validation_split=0.2)\n",
        "\n",
        "# Detect outliers\n",
        "def detect_outliers(model, image):\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    output = model.predict(image)\n",
        "    input_image = image.reshape(-1, 28, 28)\n",
        "    reconstructed = output.reshape(-1, 28, 28)\n",
        "    distance = np.mean(np.abs(input_image - reconstructed))\n",
        "    return distance\n",
        "\n",
        "# Streamlit app\n",
        "def main():\n",
        "    # Title\n",
        "    st.title(\"Outlier Detection using Streamlit and TensorFlow\")\n",
        "\n",
        "    # Download dataset\n",
        "    url = \"https://raw.githubusercontent.com/susanli2016/us-accident-data/master/accidents.npz\"\n",
        "    filename = \"accidents.npz\"\n",
        "    download_dataset(url, filename)\n",
        "\n",
        "    # Load dataset\n",
        "    images, labels = load_dataset(filename)\n",
        "    train_data, test_data = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build autoencoder model\n",
        "    input_shape = (28, 28, 1)\n",
        "    model = build_autoencoder(input_shape)\n",
        "\n",
        "    # Train autoencoder model\n",
        "    batch_size = 32\n",
        "    train_autoencoder(model, (train_data, train_data), batch_size)\n",
        "    model.load_weights('autoencoder_weights.h5')\n",
        "\n",
        "    # Upload image\n",
        "    st.header(\"Upload a grayscale image of 28x28 pixels\")\n",
        "    uploaded_image = st.file_uploader(\"Choose an image\", type=\"jpg\")\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Display uploaded image\n",
        "        image = Image.open(uploaded_image)\n",
        "        image = image.convert('L')\n",
        "        image = image.resize((28, 28))\n",
        "        st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "        # Detect outliers and show result\n",
        "        distance = detect_outliers(model, np.expand_dims(np.array(image), axis=-1))\n",
        "        st.write(\"\")\n",
        "        st.write(\"### Outlier Detection Result\")\n",
        "        st.write(\"The reconstruction error is:\", round(distance, 3))\n",
        "\n",
        "        # Show whether the image is an outlier or not\n",
        "        if distance > 0.2:\n",
        "            st.write(\"The image is an outlier!\")\n",
        "        else:\n",
        "            st.write(\"The image is not an outlier!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "_UcJyM7-ZJkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSCb7PRzZKOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}