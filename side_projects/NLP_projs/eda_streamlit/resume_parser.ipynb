{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMu5yK+ux2Zkwekt0P0e0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/resume_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Description:\n",
        "\n",
        "To create a web app to deploy a resume parser model using Streamlit, you need to have the following:\n",
        "\n",
        "A trained resume parser model (e.g., using Named Entity Recognition or Information Extraction techniques)\n",
        "Streamlit library installed in your Python environment.\n",
        "\n",
        "Here's a sample Streamlit app code for deploying a resume parser model:\n"
      ],
      "metadata": {
        "id": "OzKOORA9fsD2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoKzfSnke-IS"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the model\n",
        "model = pipeline('feature-extraction', model='cardiffnlp/twitter-roberta-base-sentence-embedding')\n",
        "\n",
        "def parse_resume(uploaded_file):\n",
        "    # Load the resume file\n",
        "    text = uploaded_file.read().decode('utf-8')\n",
        "\n",
        "    # Extract features using the model\n",
        "    features = model(text.split('.'))[0]\n",
        "\n",
        "    # Create a DataFrame to store the extracted features\n",
        "    df = pd.DataFrame(features, columns=[\n",
        "        'feature_' + str(i) for i in range(len(features))\n",
        "    ])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Title of the app\n",
        "st.title('Resume Parser Web App')\n",
        "\n",
        "# Upload a file\n",
        "uploaded_file = st.file_uploader(\"Upload a Resume (.txt or .pdf)\", type=[\"txt\", \"pdf\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Parse the resume\n",
        "    parsed_resume = parse_resume(uploaded_file)\n",
        "\n",
        "    # Display the parsed resume\n",
        "    st.subheader('Parsed Resume Features:')\n",
        "    st.write(parsed_resume)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes a Streamlit app that allows users to upload a resume file (either .txt or .pdf format) and then parses the resume using a pre-trained feature extraction model. The extracted features are displayed in a table format.\n",
        "\n",
        "Please note that this is just a basic example. You can customize the app to include more functionalities, such as saving the parsed results, adding a user interface, or integrating with other services."
      ],
      "metadata": {
        "id": "dHGdLWpTiUSa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-OMVF9zf1Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code adds enhancements to the previous example:\n",
        "\n",
        "The function load_model() initializes the feature extraction model.\n",
        "\n",
        "The function load_encoder() initializes a LabelEncoder object to encode the class labels.\n",
        "\n",
        "The function parse_resume() uses the feature extraction model and the LabelEncoder to parse the resume.\n",
        "\n",
        "The if uploaded_file is not None: block calls the parse_resume() function and displays the parsed resume as a table and as a JSON object.\n",
        "?\n",
        "By splitting the code into separate functions, we improve its readability and maintainability."
      ],
      "metadata": {
        "id": "hTRRLHFUkNbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def load_model():\n",
        "    model = pipeline('feature-extraction', model='cardiffnlp/twitter-roberta-base-sentence-embedding')\n",
        "    return model\n",
        "\n",
        "def load_encoder():\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.classes_ = ['Paragraph', 'Section', 'Bullet Point', 'Table']\n",
        "    return encoder\n",
        "\n",
        "def parse_resume(uploaded_file, model, encoder):\n",
        "    # Load the resume file\n",
        "    text = uploaded_file.read().decode('utf-8')\n",
        "\n",
        "    # Extract features using the model\n",
        "    features = model(text.split('.'))[0]\n",
        "\n",
        "    # Create a DataFrame to store the extracted features\n",
        "    df = pd.DataFrame(features, columns=[\n",
        "        'feature_' + str(i) for i in range(len(features))\n",
        "    ])\n",
        "\n",
        "    # Encode the class labels\n",
        "    df['label'] = encoder.transform(df['label'].tolist())\n",
        "\n",
        "    return df\n",
        "\n",
        "st.title('Resume Parser Web App')\n",
        "uploaded_file = st.file_uploader(\"Upload a Resume (.txt or .pdf)\", type=[\"txt\", \"pdf\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    model = load_model()\n",
        "    encoder = load_encoder()\n",
        "\n",
        "    parsed_resume = parse_resume(uploaded_file, model, encoder)\n",
        "\n",
        "    st.subheader('Parsed Resume Features:')\n",
        "    st.write(parsed_resume)\n",
        "\n",
        "    st.subheader('Parsed Resume as JSON:')\n",
        "    st.write(parsed_resume.to_json(orient='records'))"
      ],
      "metadata": {
        "id": "-D0WX34RkQjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}