{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuMLPOUa5a+2+MNe6aUhpI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/segmentation_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Medical images segmentation using deep learning model and streamlit and TensorFlow"
      ],
      "metadata": {
        "id": "epPjDvhUFY--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, install necessary libraries:\n",
        "\n",
        "!pip install tensorflow streamlit segmentation-models"
      ],
      "metadata": {
        "id": "Rv8eo9f7FtLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgJmkB2QFXu1"
      },
      "outputs": [],
      "source": [
        "# Now, let's create the training script (train.py):\n",
        "\n",
        "import tensorflow as tf\n",
        "from segmentation_models.models import UNet\n",
        "from segmentation_models.losses import bce_jaccard_loss\n",
        "from segmentation_models.metrics import iou_score\n",
        "import pathlib\n",
        "import segmentation_models as sm\n",
        "import numpy as np\n",
        "\n",
        "# Set up data paths\n",
        "data_dir = pathlib.Path('data/ACDC_2017_Data')\n",
        "\n",
        "# Define the model\n",
        "model = UNet(\n",
        "    'resnet34',\n",
        "    encoder_weights='imagenet',\n",
        "    classes=1,\n",
        "    activation='sigmoid'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=sm.losses.bce_jaccard_loss,\n",
        "    metrics=[iou_score]\n",
        ")\n",
        "\n",
        "# Load and preprocess the data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir / 'train',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir / 'train',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# Augment the data\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# Apply data augmentation\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# Train the model\n",
        "epochs = 50\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('model/unet_acdc.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, create the Streamlit app (app.py):\n",
        "\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model('model/unet_acdc.h5')\n",
        "\n",
        "# Define a function for prediction\n",
        "def predict(image):\n",
        "    image = image.resize((256, 256))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    prediction = model.predict(image)\n",
        "    return prediction[0]\n",
        "\n",
        "# Set up the Streamlit app\n",
        "st.set_page_config(page_title=\"Medical Image Segmentation\", page_icon=\":guardsman:\", layout=\"wide\")\n",
        "st.title(\"Medical Image Segmentation using Deep Learning\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=\"jpg\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "    prediction = predict(image)\n",
        "    st.write(\"Prediction:\", prediction)"
      ],
      "metadata": {
        "id": "NggF2sLQF054"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To train the model, run:\n",
        "\n",
        "python train.py"
      ],
      "metadata": {
        "id": "YHKEdNCVGAUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training, run the Streamlit app:\n",
        "\n",
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "KLVKhN4VGEz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates how to train a deep learning model for medical image segmentation using TensorFlow and the U-Net architecture, and how to deploy it using Streamlit. Note that you may need to adjust the code to fit your specific use case and dataset."
      ],
      "metadata": {
        "id": "DHGxRPPRGJAf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MpzWT9SXGJpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advancement\n",
        "\n",
        "To enhance the previous code and add more advanced features, we will:\n",
        "\n",
        "Implement a more sophisticated data augmentation pipeline using Albumentations.\n",
        "\n",
        "Use a learning rate scheduler to adjust the learning rate during training.\n",
        "Add a callback to save the best model based on validation loss.\n",
        "Implement a sliding window approach for inference on high-resolution images\n",
        "\n",
        "First, install the necessary libraries:"
      ],
      "metadata": {
        "id": "xV56gG4BIKY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations tensorflow-addons"
      ],
      "metadata": {
        "id": "vZFaVjxHINIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from segmentation_models.models import UNet\n",
        "from segmentation_models.losses import bce_jaccard_loss\n",
        "from segmentation_models.metrics import iou_score\n",
        "import pathlib\n",
        "import segmentation_models as sm\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "from tensorflow_addons.optimizers import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Set up data paths\n",
        "data_dir = pathlib.Path('data/ACDC_2017_Data')\n",
        "\n",
        "# Define the model\n",
        "model = UNet(\n",
        "    'resnet34',\n",
        "    encoder_weights='imagenet',\n",
        "    classes=1,\n",
        "    activation='sigmoid'\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=sm.losses.bce_jaccard_loss,\n",
        "    metrics=[iou_score]\n",
        ")\n",
        "\n",
        "# Define data augmentation\n",
        "data_augmentation = A.Compose([\n",
        "    A.RandomRotate(limit=15),\n",
        "    A.RandomBrightness(limit=0.1),\n",
        "    A.RandomContrast(limit=0.1),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),\n",
        "    A.Cutout(num_holes=8, max_hole_size=64, p=0.5),\n",
        "])\n",
        "\n",
        "# Load and preprocess the data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir / 'train',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir / 'train',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "# Apply data augmentation\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(image=x)['image'], y))\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "lr_scheduler = LearningRateScheduler(schedule=lambda epoch: 1e-4 * 0.1**(epoch / 10))\n",
        "\n",
        "# Define a callback to save the best model based on validation loss\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='model/unet_acdc_best.h5',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "epochs = 100\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=[lr_scheduler, checkpoint_callback]\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save('model/unet_acdc_final.h5')"
      ],
      "metadata": {
        "id": "0cGtnvOWIjZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the app.py script to include a sliding window approach for inference on high-resolution images:"
      ],
      "metadata": {
        "id": "iSubkyolImVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Load the best model\n",
        "model = tf.keras.models.load_model('model/unet_acdc_best.h5')\n",
        "\n",
        "# Define a function for prediction with a sliding window\n",
        "def predict_sliding_window(image, window_size, step_size):\n",
        "    prediction = np.zeros(image.shape[:2])\n",
        "    for i in range(0, image.shape[0] - window_size[0], step_size):\n",
        "        for j in range(0, image.shape[1] - window_size[1], step_size):\n",
        "            window = image[i:i + window_size[0], j:j + window_size[1]]\n",
        "            window = cv2.resize(window, (256, 256))\n",
        "            window = np.expand_dims(window, axis=0)\n",
        "            window = window / 255.0\n",
        "            segmentation = model.predict(window)\n",
        "            prediction[i:i + window_size[0], j:j + window_size[1]] = segmentation[0]\n",
        "    return prediction\n",
        "\n",
        "# Define a function for prediction\n",
        "def predict(image):\n",
        "    image = image.resize((256, 256))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    prediction = model.predict(image)\n",
        "    return prediction[0]\n",
        "\n",
        "# Set up the Streamlit app\n",
        "st.set_page_config(page_title=\"Medical Image Segmentation\", page_icon=\":guardsman:\", layout=\"wide\")\n",
        "st.title(\"Medical Image Segmentation using Deep Learning\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=\"jpg\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "    # Sliding window for high-resolution images\n",
        "    if image.height > 256 or image.width > 256:\n",
        "        st.write(\"Processing high-resolution image...\")\n",
        "        window_size = (256, 256)\n",
        "        step_size = (128, 128)\n",
        "        prediction = predict_sliding_window(np.array(image), window_size, step_size)\n",
        "    else:\n",
        "        prediction = predict(image)\n",
        "\n",
        "    # Display the prediction\n",
        "    prediction_image = Image.fromarray((prediction * 255).astype(np.uint8))\n",
        "    st.image(prediction_image, caption='Prediction', use_column_width=True)"
      ],
      "metadata": {
        "id": "Cpf-f11TIno3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Load the best model\n",
        "model = tf.keras.models.load_model('model/unet_acdc_best.h5')\n",
        "\n",
        "# Define a function for prediction with a sliding window\n",
        "def predict_sliding_window(image, window_size, step_size):\n",
        "    prediction = np.zeros(image.shape[:2])\n",
        "    for i in range(0, image.shape[0] - window_size[0], step_size):\n",
        "        for j in range(0, image.shape[1] - window_size[1], step_size):\n",
        "            window = image[i:i + window_size[0], j:j + window_size[1]]\n",
        "            window = cv2.resize(window, (256, 256))\n",
        "            window = np.expand_dims(window, axis=0)\n",
        "            window = window / 255.0\n",
        "            segmentation = model.predict(window)\n",
        "            prediction[i:i + window_size[0], j:j + window_size[1]] = segmentation[0]\n",
        "    return prediction\n",
        "\n",
        "# Define a function for prediction\n",
        "def predict(image):\n",
        "    image = image.resize((256, 256))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    prediction = model.predict(image)\n",
        "    return prediction[0]\n",
        "\n",
        "# Set up the Streamlit app\n",
        "st.set_page_config(page_title=\"Medical Image Segmentation\", page_icon=\":guardsman:\", layout=\"wide\")\n",
        "st.title(\"Medical Image Segmentation using Deep Learning\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=\"jpg\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "    # Sliding window for high-resolution images\n",
        "    if image.height > 256 or image.width > 256:\n",
        "        st.write(\"Processing high-resolution image...\")\n",
        "        window_size = (256, 256)\n",
        "        step_size = (128, 128)\n",
        "        prediction = predict_sliding_window(np.array(image), window_size, step_size)\n",
        "    else:\n",
        "        prediction = predict(image)\n",
        "\n",
        "    # Display the prediction\n",
        "    prediction_image = Image.fromarray((prediction * 255).astype(np.uint8))\n",
        "    st.image(prediction_image, caption='Prediction', use_column_width=True)"
      ],
      "metadata": {
        "id": "u4rVcC6wi6vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Load the best model\n",
        "model = tf.keras.models.load_model('model/unet_acdc_best.h5')\n",
        "\n",
        "# Define a function for prediction with a sliding window\n",
        "def predict_sliding_window(image, window_size, step_size):\n",
        "    prediction = np.zeros(image.shape[:2])\n",
        "    for i in range(0, image.shape[0] - window_size[0], step_size):\n",
        "        for j in range(0, image.shape[1] - window_size[1], step_size):\n",
        "            window = image[i:i + window_size[0], j:j + window_size[1]]\n",
        "            window = cv2.resize(window, (256, 256))\n",
        "            window = np.expand_dims(window, axis=0)\n",
        "            window = window / 255.0\n",
        "            segmentation = model.predict(window)\n",
        "            prediction[i:i + window_size[0], j:j + window_size[1]] = segmentation[0]\n",
        "    return prediction\n",
        "\n",
        "# Define a function for prediction\n",
        "def predict(image):\n",
        "    image = image.resize((256, 256))\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0\n",
        "    prediction = model.predict(image)\n",
        "    return prediction[0]\n",
        "\n",
        "# Set up the Streamlit app\n",
        "st.set_page_config(page_title=\"Medical Image Segmentation\", page_icon=\":guardsman:\", layout=\"wide\")\n",
        "st.title(\"Medical Image Segmentation using Deep Learning\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=\"jpg\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "    # Sliding window for high-resolution images\n",
        "    if image.height > 256 or image.width > 256:\n",
        "        st.write(\"Processing high-resolution image...\")\n",
        "        window_size = (256, 256)\n",
        "        step_size = (128, 128)\n",
        "        prediction = predict_sliding_window(np.array(image), window_size, step_size)\n",
        "    else:\n",
        "        prediction = predict(image)\n",
        "\n",
        "    # Display the prediction\n",
        "    prediction_image = Image.fromarray((prediction * 255).astype(np.uint8))\n",
        "    st.image(prediction_image, caption='Prediction', use_column_width=True)"
      ],
      "metadata": {
        "id": "2qnroUh32Vce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script now includes a sliding window approach for inference on high-resolution images. The script checks if the uploaded image's height or width is greater than 256 pixels. If so, it uses the predict_sliding_window function to generate the prediction. Otherwise, it uses the predict function to generate the prediction. The prediction is then displayed using Streamlit.\n",
        "\n",
        "To run the app, execute the following command in the terminal:"
      ],
      "metadata": {
        "id": "J2-zcLBRI7v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run app.py"
      ],
      "metadata": {
        "id": "rNqhu_4aI8jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will start a local Streamlit server, and you can access the app by opening a web browser and navigating to http://localhost:8501."
      ],
      "metadata": {
        "id": "LZG3E0ZYJAlk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zpNdWkVvJBmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}