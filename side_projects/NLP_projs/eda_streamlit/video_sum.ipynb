{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy/zSAF8dukNnnAeFS+5kN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/video_sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video summarization using reinforcement learning, GANs and streamlit and tensorflow\n",
        "\n",
        "The file **video_sum.ipynb** contains an existing code for video summarization using reinforcement learning, GANs, Streamlit, and TensorFlow, we can add a reward system based on user feedback to improve the summarization model over time. Here is an example of how you could implement this:\n",
        "\n",
        "First, let's import the necessary libraries:W"
      ],
      "metadata": {
        "id": "woO0Pv8jKs37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from html2image import Html2Image\n",
        "import youtube_transcript_api\n",
        "import streamlit as st\n",
        "from langchain import OpenAI\n",
        "from llama_index import StorageContext, load_index_from_storage\n",
        "from llama_index import VectorStoreIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper, ServiceContext\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "1-T8NjfkK-go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's define the reward function based on user feedback:\n",
        "\n"
      ],
      "metadata": {
        "id": "hJT0BJBuLaLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_function(user_feedback, original_summary, generated_summary):\n",
        "    if user_feedback == \"like\":\n",
        "        reward = 1\n",
        "    elif user_feedback == \"dislike\":\n",
        "        reward = -1\n",
        "    else:\n",
        "        reward = 0\n",
        "\n",
        "    if original_summary is not None and generated_summary is not None:\n",
        "        rouge_score = nltk.translate.meteor_score.meteor_score([original_summary], [generated_summary])\n",
        "        reward += (0.5 * rouge_score)\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "id": "stf9-MBLLcCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a function to train the model using reinforcement learning:\n",
        "\n"
      ],
      "metadata": {
        "id": "yG7f27MZLhfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(episodes, max_seq_length, learning_rate, batch_size, summary_dir):\n",
        "    # Load the dataset\n",
        "    documents = load_documents(summary_dir)\n",
        "\n",
        "    # Preprocess the data\n",
        "    tokenizer, sequences, input_sequences, target_sequences = preprocess_data(documents, max_seq_length)\n",
        "\n",
        "    # Define the model architecture\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    lstm = LSTM(units=256, return_sequences=True)(inputs)\n",
        "    lstm = LSTM(units=256)(lstm)\n",
        "    outputs = Dense(units=1, activation='linear')(lstm)\n",
        "\n",
        "    # Compile the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "    # Train the model\n",
        "    for episode in range(episodes):\n",
        "        print(f'Episode: {episode}')\n",
        "        summary_pairs = generate_summary_pairs(documents)\n",
        "        random.shuffle(summary_pairs)\n",
        "\n",
        "        for i in range(0, len(summary_pairs), batch_size):\n",
        "            minibatch = summary_pairs[i:i+batch_size]\n",
        "            user_feedback = [random.choice(['like', 'dislike']) for _ in range(len(minibatch))]\n",
        "            original_summaries = [pair[0] for pair in minibatch]\n",
        "            generated_summaries = [pair[1] for pair in minibatch]\n",
        "\n",
        "            rewards = np.array([reward_function(fb, os.path.basename(os.path.splitext(os.path.split(s)[1])[0]), gs) for fb, s, gs in zip(user_feedback, original_summaries, generated_summaries)])\n",
        "            target_floats = rewards + model.predict(input_sequences[i:i+batch_size])\n",
        "            model.train_on_batch(input_sequences[i:i+batch_size], target_floats)\n",
        "\n",
        "    # Save the model\n",
        "    model.save('video_summarization_model.h5')"
      ],
      "metadata": {
        "id": "1njsxq-lLiSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the function to generate summary pairs for training:\n",
        "\n"
      ],
      "metadata": {
        "id": "4gV_HXiGLs2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary_pairs(documents):\n",
        "    summary_pairs = []\n",
        "    for document in documents:\n",
        "        # Generate a summary using the original model\n",
        "        original_summary = generate_summary(document)\n",
        "\n",
        "        # Generate a summary using the reinforcement learning model\n",
        "        generated_summary = generate_summary_with_reinforcement_learning(document)\n",
        "\n",
        "        # Add the pair to the list\n",
        "        summary_pairs.append((original_summary, generated_summary))\n",
        "\n",
        "    return summary_pairs"
      ],
      "metadata": {
        "id": "_0K4fj7yLtaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create a function to generate a summary using the original model:\n",
        "\n"
      ],
      "metadata": {
        "id": "fWROBCUiLyXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(document):\n",
        "    # Extract the video ID from the document\n",
        "    video_id = os.path.splitext(os.path.split(document)[1])[0]\n",
        "\n",
        "    # Load the transcript\n",
        "    transcript = youtube_transcript_api.get_transcript(video_id)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary = ' '.join([line['text'] for line in transcript])\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "8sAVWsVBLy70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's create a function to generate a summary using the reinforcement learning model:"
      ],
      "metadata": {
        "id": "RcFy774RL3nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary_with_reinforcement_learning(document):\n",
        "    # Load the model\n",
        "    model = load_model('video_summarization_model.h5')\n",
        "\n",
        "    # Tokenize the document\n",
        "    tokenizer = load_tokenizer('tokenizer.json')\n",
        "    sequences = tokenizer.texts_to_sequences([document])\n",
        "\n",
        "    # Generate the summary\n",
        "    summary = ''\n",
        "    for i in range(len(sequences[0])):\n",
        "        input_sequence = sequences[0][:i+1]\n",
        "        input_sequence = np.array(input_sequence).reshape(1, -1)\n",
        "\n",
        "        # Predict the next word\n",
        "        prediction = model.predict(input_sequence)\n",
        "        next_word_index = np.argmax(prediction)\n",
        "\n",
        "        # Decode the next word\n",
        "        next_word = tokenizer.index_word[next_word_index]\n",
        "\n",
        "        # Add the next word to the summary\n",
        "        summary += next_word + ' '\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "mJwDwK_8L4Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a basic implementation of reinforcement learning for video summarization. The model is trained using a reward function that takes into account user feedback and the similarity between the original and generated summaries. The model is then used to generate summaries for new videos. Note that this implementation is simplified and may not provide the best results. For a more advanced implementation, you could consider using a pre-trained language model like GPT-3 or BERT to generate summaries.\n",
        "\n"
      ],
      "metadata": {
        "id": "B_iNWBYgL9WL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4gQKSZgL9-2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}