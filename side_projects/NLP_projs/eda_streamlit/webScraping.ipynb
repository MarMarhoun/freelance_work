{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuLVmWNRJ6pE1rOD7Wt3c9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarMarhoun/freelance_work/blob/main/side_projects/NLP_projs/eda_streamlit/webScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to web scrapping and crawling using streamlit"
      ],
      "metadata": {
        "id": "HX4o2VQyZP9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project description:\n",
        "\n",
        "The code for web scraping and crawling using Streamlit, you can create a user interface that allows users to input search queries and filter results. Here's an example of how you can modify the existing code to incorporate Streamlit:"
      ],
      "metadata": {
        "id": "3lf-BccIUETj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Module for interacting with the operating system\n",
        "import time  # Module for time-related operations\n",
        "import ujson  # Module for working with JSON data\n",
        "from random import randint  # Module for generating random numbers\n",
        "from typing import Dict, List, Any  # Type hinting imports\n",
        "\n",
        "import requests  # Library for making HTTP requests\n",
        "from bs4 import BeautifulSoup  # Library for parsing HTML data\n",
        "from selenium import webdriver  # Library for browser automation\n",
        "from selenium.common.exceptions import NoSuchElementException  # Exception for missing elements\n",
        "from webdriver_manager.chrome import ChromeDriverManager  # Driver manager for Chrome (We are using Chromium based )\n",
        "import streamlit as st  # Streamlit library for building web apps\n",
        "\n",
        "def initCrawlerScraper(seed, max_profiles=500):\n",
        "    # Initialize driver for Chrome\n",
        "    webOpt = webdriver.ChromeOptions()\n",
        "    webOpt.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
        "    webOpt.add_argument('--ignore-certificate-errors')\n",
        "    webOpt.add_argument('--incognito')\n",
        "    webOpt.headless = True\n",
        "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=webOpt)\n",
        "    driver.get(seed)  # Start with the original link\n",
        "\n",
        "    links = []  # Array with pureportal profiles URL\n",
        "    pub_data = []  # To store publication information for each pureportal profile\n",
        "\n",
        "    nextLink = driver.find_element_by_css_selector(\".nextLink\").is_enabled()  # Check if the next page link is enabled\n",
        "    print(\"Crawler has begun...\")\n",
        "    while (nextLink):\n",
        "        page = driver.page_source\n",
        "        # XML parser to parse each URL\n",
        "        bs = BeautifulSoup(page, \"lxml\")  # Parse the page source using BeautifulSoup\n",
        "\n",
        "        # Extracting exact URL by spliting string into list\n",
        "        for link in bs.findAll('a', class_='link person'):\n",
        "            url = str(link)[str(link).find('<https://pureportal.coventry.ac.uk/en/persons/'):].split('>\"')\n",
        "            links.append(url[0])\n",
        "\n",
        "        # Click on Next button to visit next page\n",
        "        try:\n",
        "            if driver.find_element_by_css_selector(\".nextLink\"):\n",
        "                element = driver.find_element_by_css_selector(\".nextLink\")\n",
        "                driver.execute_script(\"arguments[0].click();\", element)\n",
        "            else:\n",
        "                nextLink = False\n",
        "        except NoSuchElementException:\n",
        "            break\n",
        "\n",
        "        # Check if the maximum number of profiles is reached\n",
        "        if len(links) >= max_profiles:\n",
        "            break\n",
        "\n",
        "    print(\"Crawler has found \", len(links), \" pureportal profiles\")\n",
        "    write_authors(links, 'Authors_URL.txt')  # Write the authors' URLs to a file\n",
        "\n",
        "    print(\"Scraping publication data for \", len(links), \" pureportal profiles...\")\n",
        "    count = 0\n",
        "    for link in links:\n",
        "        # Visit each link to get data\n",
        "        time.sleep(1)\n",
        "        driver.get(link)\n",
        "        try:\n",
        "            if driver.find_elements_by_css_selector(\".portal_link.btn-primary.btn-large\"):\n",
        "                element = driver.find_elements_by_css_selector(\".portal_link.btn-primary.btn-large\")\n",
        "                for a in element:\n",
        "                    if \"research output\".lower() in a.text.lower():\n",
        "                        driver.execute_script(\"arguments[0].click();\", a)\n",
        "                        driver.get(driver.current_url)\n",
        "                        # Get name of Author\n",
        "                        name = driver.find_element_by_css_selector(\"div[class='header person-details']>h1\")\n",
        "                        r = requests.get(driver.current_url)\n",
        "                        # Parse all the data via BeautifulSoup\n",
        "                        soup = BeautifulSoup(r.content, 'lxml')\n",
        "\n",
        "                        # Extracting publication name, publication url, date and CU Authors\n",
        "                        table = soup.find('ul', attrs={'class': 'list-results'})\n",
        "                        if table != None:\n",
        "                            for row in table.findAll('div', attrs={'class': 'result-container'}):\n",
        "                                data = {}\n",
        "                                data['name'] = row.h3.a.text\n",
        "                                data['pub_url'] = row.h3.a['href']\n",
        "                                date = row.find(\"span\", class_=\"date\")\n",
        "\n",
        "                                rowitem = row.find_all(['div'])\n",
        "                                span = row.find_all(['span'])\n",
        "                                data['cu_author'] = name.text\n",
        "                                data['date'] = date.text\n",
        "                                print(\"Publication Name :\", row.h3.a.text)\n",
        "                                print(\"Publication URL :\", row.h3.a['href'])\n",
        "                                print(\"CU Author :\", name.text)\n",
        "                                print(\"Date :\", date.text)\n",
        "                                print(\"\\\\n\")\n",
        "                                pub_data.append(data)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    print(\"Crawler has scrapped data for \", len(pub_data), \" pureportal publications\")\n",
        "    driver.quit()\n",
        "    # Writing all the scraped results in a file with JSON format\n",
        "    with open('scraper_results.json', 'w') as f:\n",
        "        ujson.dump(pub_data, f)\n",
        "\n",
        "def main():\n",
        "    st.title(\"Web Scraping and Crawling with Streamlit\")\n",
        "\n",
        "    search_query = st.text_input(\"Enter search query:\")\n",
        "    max_results = st.number_input(\"Maximum number of results:\", value=10)\n",
        "\n",
        "    if st.button(\"Search\"):\n",
        "        if not search_query:\n",
        "            st.warning(\"Please enter a search query.\")\n",
        "        else:\n",
        "            pub_data = initCrawlerScraper(search_query, max_results)\n",
        "            st.write(\"Results:\")\n",
        "            for link in pub_data:\n",
        "                st.markdown(f\"[{link}]({link})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Sqgrkr2tU_4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyOURRhBVOg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to web scrapping and crawling using Flask and Django\n",
        "\n",
        "To create a web scraping and crawling application using Python, Flask, Beautiful Soup, and Requests, follow the steps below:\n",
        "\n",
        "Install the required libraries:"
      ],
      "metadata": {
        "id": "ie6kyNA_ZL8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask beautifulsoup4"
      ],
      "metadata": {
        "id": "F8ZOuLRTZOab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new Flask application (app.py):\n"
      ],
      "metadata": {
        "id": "kMqzpCDBZfEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request, url_for\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    if request.method == 'POST':\n",
        "        url = request.form['url']\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract data using Beautiful Soup (replace with your own selectors)\n",
        "        title = soup.select_one('h1').text.strip()\n",
        "        description = soup.select_one('meta[name=description]')['content'].strip()\n",
        "\n",
        "        return render_template('index.html', url=url, title=title, description=description)\n",
        "\n",
        "    return render_template('index.html')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "4sXDNXbyZhVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an HTML template (templates/index.html):\n",
        "/"
      ],
      "metadata": {
        "id": "ayttXQo7ZmqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Web Scraping and Crawling with Flask</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Web Scraping and Crawling with Flask</h1>\n",
        "\n",
        "    <form method=\"post\">\n",
        "        <label for=\"url\">Enter URL:</label>\n",
        "        <input type=\"text\" name=\"url\" id=\"url\" required>\n",
        "        <button type=\"submit\">Scrape</button>\n",
        "    </form>\n",
        "\n",
        "    {% if title %}\n",
        "    <h2>{{ title }}</h2>\n",
        "    <p>{{ description }}</p>\n",
        "    {% endif %}\n",
        "</body>\n",
        "</html>"
      ],
      "metadata": {
        "id": "vq4sF8dSZnTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Flask application:\n",
        "\n",
        "python app.py"
      ],
      "metadata": {
        "id": "-m1splC4Zrzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the application in your web browser at http://127.0.0.1:5000/. Enter a URL to scrape and click \"Scrape\". The application will display the title and description of the provided URL.\n",
        "\n",
        "This is a basic example of web scraping and crawling using Flask, Beautiful Soup, and Requests. You can further customize the code to extract more data or implement additional features like following links or handling JavaScript-rendered content.\n",
        "\n",
        "### Advanced Features and Enhancements\n",
        "For advanced features and enhancements, you can consider the following options:\n",
        "\n",
        "Handle different content types (HTML, JSON, XML) using different parsing techniques.\n",
        "\n",
        "Store scraped data in a database (e.g., SQLite, MySQL, MongoDB) for future use.\n",
        "\n",
        "Implement web crawling techniques to follow links and recursively scrape data.\n",
        "\n",
        "Add authentication and authorization features to restrict access to the scraping functionality.\n",
        "\n",
        "Enhance the user interface using front-end libraries (e.g., Bootstrap, React, Vue.js) and improve the user experience.\n",
        "\n",
        "Optimize the performance of the application by using asynchronous techniques (e.g., with asyncio and aiohttp) or parallel processing (e.g., with concurrent.futures).\n",
        "\n",
        "Consider implementing unit tests and integration tests to ensure the stability and reliability of the application.\n",
        "\n",
        "Remember to replace the example selectors used in the code with your own specific selectors for the target websites. Additionally, be aware of the legality and ethical considerations when web scraping, and ensure that you are complying with any applicable laws and terms of service."
      ],
      "metadata": {
        "id": "SgB7G4R4ZyLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, render_template, request, url_for\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from flask_wtf import FlaskForm\n",
        "from wtforms import StringField, SubmitField\n",
        "from wtforms.validators import DataRequired, URL\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['SECRET_KEY'] = 'your-secret-key'\n",
        "\n",
        "class UrlForm(FlaskForm):\n",
        "    url = StringField('url', validators=[DataRequired(), URL()])\n",
        "    submit = SubmitField('Scrape')\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def index():\n",
        "    form = UrlForm()\n",
        "    if form.validate_on_submit():\n",
        "        url = form.url.data\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract data using Beautiful Soup (replace with your own selectors)\n",
        "        title = soup.select_one('h1').text.strip()\n",
        "        description = soup.select_one('meta[name=description]')['content'].strip()\n",
        "\n",
        "        return render_template('index.html', title=title, description=description)\n",
        "\n",
        "    return render_template('index.html', form=form)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "wpEX317_Zz0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to web scrapping and crawling using Django\n",
        "\n",
        "Web scraping and crawling using D, you can create a Django application that utilizes Scrapy for scraping data and integrates it into Django views to return the scraped data as JSON responses. Here's a step-by-step:\n",
        "\n",
        "Create a new Django project and app:"
      ],
      "metadata": {
        "id": "4Cjo1iQlaOsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "django-admin startproject myproject\n",
        "python manage.py startapp scrapers"
      ],
      "metadata": {
        "id": "ssuzvdNfe-9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Scrapy as a dependency:\n"
      ],
      "metadata": {
        "id": "CMPTijJGfGME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "id": "C8OAHU_rfHHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new Scrapy spider for scraping data. For example, to scrape data from the Hacker News website, create a file named hacker_news_spider.py inside the spiders directory of your Scrapy project:"
      ],
      "metadata": {
        "id": "nwUJq3_QfMXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "\n",
        "class HackerNewsSpider(scrapy.Spider):\n",
        "    name = \"hacker_news\"\n",
        "    start_urls = [\n",
        "        \"https://news.ycombinator.com/\",\n",
        "    ]\n",
        "\n",
        "    def parse(self, response):\n",
        "        for article in response.css(\"tr.athing\"):\n",
        "            yield {\n",
        "                \"title\": article.css(\"a.storylink::text\").get(),\n",
        "                \"url\": article.css(\"a.storylink::attr(href)\").get(),\n",
        "                \"votes\": int(article.css(\"span.score::text\").re_first(r\"\\d+\"))\n",
        "            }\n",
        "        next_page = response.css(\"a.morelink::attr(href)\").get()\n",
        "        if next_page is not None:\n",
        "            yield response.follow(next_page, self.parse)"
      ],
      "metadata": {
        "id": "gNdCN4LVfOFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Django view that will execute the Scrapy spider and return the scraped data as a JSON response:"
      ],
      "metadata": {
        "id": "ySIALKsgfTTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from django.http import JsonResponse\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapers.spiders.hacker_news_spider import HackerNewsSpider\n",
        "\n",
        "def scrape_hacker_news(request):\n",
        "    process = CrawlerProcess(settings={\n",
        "        \"FEEDS\": {\n",
        "            \"items.json\": {\"format\": \"json\"},\n",
        "        },\n",
        "    })\n",
        "    process.crawl(HackerNewsSpider)\n",
        "    process.start()\n",
        "    with open(\"items.json\", \"r\") as f:\n",
        "        data = f.read()\n",
        "    return JsonResponse(data, safe=False)"
      ],
      "metadata": {
        "id": "Va6ZHxGnfT78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a URL route for the new view:\n"
      ],
      "metadata": {
        "id": "ClZ5vb6GfX5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from django.urls import path\n",
        "from .views import scrape_hacker_news\n",
        "\n",
        "urlpatterns = [\n",
        "    path(\"scrape-hacker-news/\", scrape_hacker_news, name=\"scrape_hacker_news\"),\n",
        "]"
      ],
      "metadata": {
        "id": "V8xR9oRIfYhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can visit the URL /scrape-hacker-news/ in your Django application to execute the Scrapy spider and return the scraped data as a JSON response.\n",
        "\n",
        "For more advanced use cases, you can consider using Celery or Django-Crontab to schedule and run the Scrapy spiders in the background. Additionally, you can use Django's ORM to store the scraped data in a database and create more sophisticated views and templates for displaying the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "tzqL3K7Nfcln"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwm8flL9fdRl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}